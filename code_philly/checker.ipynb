{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Youtube_Transcript import y_transcript\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import CodeSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import os\n",
    "os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\"\n",
    "from git import Repo\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from RAG import RAG,RAG_query\n",
    "from vector_storing import vector_storing\n",
    "from vector_load import vector_load\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import VectorStoreIndex,SummaryIndex,StorageContext,SimpleKeywordTableIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "def checker(data):\n",
    "\n",
    "    # response_ext = {'url':'https://www.youtube.com/watch?v=RLUKn6dBhxU','video_id':'RLUKn6dBhxU','video_title':'7 Must-have Apps for Small Businesses (that we support)'}\n",
    "    # response_ext = {'url':'https://github.com/stanford-futuredata/FrugalGPT','repo_name':'FrugalGPT'}\n",
    "    response_ext = {'url':'https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_refit_callable.html#sphx-glr-auto-examples-model-selection-plot-grid-search-refit-callable-py'}\n",
    "\n",
    "\n",
    "    if 'youtube' in response_ext['url'].lower():\n",
    "        # print('hi')\n",
    "        transcript_text = y_transcript(response_ext['video_id'])\n",
    "        if 'video_title' in response_ext:\n",
    "            document = Document(text=transcript_text,\n",
    "                                    metadata = {\"video_title\": response_ext['video_title'], \"video_url\": response_ext['url']})\n",
    "        else:\n",
    "            document = Document(text=transcript_text,\n",
    "                                    metadata = {\"video_url\": response_ext['url']})\n",
    "        documents = [document]\n",
    "        # print(documents)\n",
    "\n",
    "    elif 'github' in response_ext['url'].lower():\n",
    "        print(' in github')\n",
    "        repo_name = response_ext['repo_name']\n",
    "    \n",
    "        repo_path = \".\\\\{}\\\\\".format(repo_name)\n",
    "        repo = Repo.clone_from(response_ext['url'], to_path=repo_path)\n",
    "        loader = GenericLoader.from_filesystem(\n",
    "            repo_path,\n",
    "            glob=\"**/*\",\n",
    "            suffixes=[\".py\"],\n",
    "            exclude=[\"**/non-utf8-encoding.py\",\"setup.py\"],\n",
    "            parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    "        )\n",
    "        texts = python_splitter.split_documents(documents)\n",
    "        db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "        # create collection\n",
    "        chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "        # assign chroma as the vector_store to the context\n",
    "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "        # create your index\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            [documents], storage_context=storage_context\n",
    "        )\n",
    "\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "            vector_store, storage_context=storage_context\n",
    "        )\n",
    "        \n",
    "        code_query_engine = index.as_query_engine()\n",
    "\n",
    "\n",
    "\n",
    "        response = code_query_engine.query('can you list all the models that the author used and give the github repo of the code file where he used all the models')\n",
    "        print(str(response))\n",
    "        print()\n",
    "\n",
    "        # response = code_query_engine.query('Can I get the repo link of scoring code')\n",
    "        # print(str(response))\n",
    "        # # RAG(nodes)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        loader = UnstructuredURLLoader(urls=[response_ext['url']],  show_progress_bar=True)\n",
    "        loader = UnstructuredURLLoader(urls=[response_ext['url']],  show_progress_bar=True)\n",
    "        documents = loader.load()\n",
    "        document = Document(text=documents[0].page_content)\n",
    "        parser = SentenceSplitter(chunk_size=1024)\n",
    "        nodes = parser.get_nodes_from_documents([document])\n",
    "        index = vector_storing(\"./embeddings/top_k\",\"text-embedding-3-small\",\"vector_store\",nodes,\"VectorStoreIndex\")\n",
    "        engione=index.as_query_engine()\n",
    "        response = engione.query(\"what is candidate_idx ? can you explain ?\")\n",
    "        print(str(response))\n",
    "\n",
    "        # print(data)\n",
    "        print(nodes)\n",
    "\n",
    "\n",
    "\n",
    "    # parser = SentenceSplitter(chunk_size=1024)\n",
    "    # nodes = parser.get_nodes_from_documents(documents)\n",
    "    # # print(nodes)\n",
    "\n",
    "\n",
    "\n",
    "    # RAG(nodes)\n",
    "    # response = RAG_query('can you summarise the entire video')\n",
    "    # print(response)\n",
    "\n",
    "# do webpage text stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_idx is the index of a model that has its test score greater than or equal to a specified threshold value. It is the index of a candidate model that meets the criteria of having a test score within 1 standard deviation of the best mean test score.\n",
      "[TextNode(id_='1fb22c23-3351-460b-a5da-e614a913cd66', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b130d70c-cab8-4c80-8f83-2f4a29a50d03', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='c9fbe2189963bbfdc56008a1ac92226658e586b794d23f3577a71c740ef72259'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4e9433b0-bf73-4c3a-945a-cfde38bb0151', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='65eb2542cf1369e9a2a66f2c1517ea30bc8f9222d8a9cc74a40615a0267ea44f')}, text='Install\\n\\nUser Guide\\n\\nAPI\\n\\nExamples\\n\\nCommunity\\n\\nGetting Started\\n\\nTutorial\\n\\nWhat\\'s new\\n\\nGlossary\\n\\nDevelopment\\n\\nFAQ\\n\\nSupport\\n\\nRelated packages\\n\\nRoadmap\\n\\nGovernance\\n\\nAbout us\\n\\nGitHub\\n\\nOther Versions and Download\\n\\nMore\\n          \\n              Getting Started\\n              Tutorial\\n              What\\'s new\\n              Glossary\\n              Development\\n              FAQ\\n              Support\\n              Related packages\\n              Roadmap\\n              Governance\\n              About us\\n              GitHub\\n              Other Versions and Download\\n\\nPrev\\n\\nUp\\n\\nNext\\n\\nscikit-learn 1.4.2\\n\\nOther versions\\n\\nPlease cite us if you use the software.\\n\\nBalance model complexity and cross-validated score\\n\\nNote\\n\\nGo to the end\\nto download the full example code or to run this example in your browser via JupyterLite or Binder\\n\\nBalance model complexity and cross-validated score¶\\n\\nThis example balances model complexity and cross-validated score by\\nfinding a decent accuracy within 1 standard deviation of the best accuracy\\nscore while minimising the number of PCA components [1].\\n\\nThe figure shows the trade-off between cross-validated score and the number\\nof PCA components. The balanced case is when n_components=10 and accuracy=0.88,\\nwhich falls into the range within 1 standard deviation of the best accuracy\\nscore.\\n\\n[1] Hastie, T., Tibshirani, R.,, Friedman, J. (2001). Model Assessment and\\nSelection. The Elements of Statistical Learning (pp. 219-260). New York,\\nNY, USA: Springer New York Inc..\\n\\nThe best_index_ is 2\\nThe n_components selected is 10\\nThe corresponding accuracy score is 0.88\\n\\n# Author: Wenhao Zhang <wenhaoz@ucla.edu>\\n\\nimport\\n\\nmatplotlib.pyplot\\n\\nas\\n\\nplt\\n\\nimport\\n\\nnumpy\\n\\nas\\n\\nnp\\n\\nfrom\\n\\nsklearn.datasets\\n\\nimport\\n\\nload_digits\\n\\nfrom\\n\\nsklearn.decomposition\\n\\nimport\\n\\nPCA\\n\\nfrom\\n\\nsklearn.model_selection\\n\\nimport\\n\\nGridSearchCV\\n\\nfrom\\n\\nsklearn.pipeline\\n\\nimport\\n\\nPipeline\\n\\nfrom\\n\\nsklearn.svm\\n\\nimport\\n\\nLinearSVC\\n\\ndef\\n\\nlower_bound\\n\\ncv_results\\n\\n):\\n\\n\"\"\"\\n\\nCalculate the lower bound within 1 standard deviation\\n\\nof the best `mean_test_scores`.\\n\\nParameters\\n\\n----------\\n\\ncv_results : dict of numpy(masked) ndarrays\\n\\nSee attribute cv_results_ of `GridSearchCV`\\n\\nReturns\\n\\n-------\\n\\nfloat\\n\\nLower bound within 1 standard deviation of the\\n\\nbest `mean_test_score`.\\n\\n\"\"\"\\n\\nbest_score_idx\\n\\nnp.argmax\\n\\ncv_results\\n\\n\"mean_test_score\"\\n\\n])\\n\\nreturn\\n\\ncv_results\\n\\n\"mean_test_score\"\\n\\n][\\n\\nbest_score_idx\\n\\ncv_results\\n\\n\"std_test_score\"\\n\\n][\\n\\nbest_score_idx\\n\\ndef\\n\\nbest_low_complexity\\n\\ncv_results\\n\\n):\\n\\n\"\"\"\\n\\nBalance model complexity with cross-validated score.\\n\\nParameters\\n\\n----------\\n\\ncv_results : dict of numpy(masked) ndarrays\\n\\nSee attribute cv_results_ of `GridSearchCV`.\\n\\nReturn\\n\\n------\\n\\nint\\n\\nIndex of a model that has the fewest PCA components\\n\\nwhile has its test score within 1 standard deviation of the best\\n\\n`mean_test_score`.\\n\\n\"\"\"', start_char_idx=0, end_char_idx=2809, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='4e9433b0-bf73-4c3a-945a-cfde38bb0151', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b130d70c-cab8-4c80-8f83-2f4a29a50d03', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='c9fbe2189963bbfdc56008a1ac92226658e586b794d23f3577a71c740ef72259'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1fb22c23-3351-460b-a5da-e614a913cd66', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='681ff13e48155a0390a93a125c535dd4ae197e2596076339bc28ad368b8a60ee')}, text='Parameters\\n\\n----------\\n\\ncv_results : dict of numpy(masked) ndarrays\\n\\nSee attribute cv_results_ of `GridSearchCV`\\n\\nReturns\\n\\n-------\\n\\nfloat\\n\\nLower bound within 1 standard deviation of the\\n\\nbest `mean_test_score`.\\n\\n\"\"\"\\n\\nbest_score_idx\\n\\nnp.argmax\\n\\ncv_results\\n\\n\"mean_test_score\"\\n\\n])\\n\\nreturn\\n\\ncv_results\\n\\n\"mean_test_score\"\\n\\n][\\n\\nbest_score_idx\\n\\ncv_results\\n\\n\"std_test_score\"\\n\\n][\\n\\nbest_score_idx\\n\\ndef\\n\\nbest_low_complexity\\n\\ncv_results\\n\\n):\\n\\n\"\"\"\\n\\nBalance model complexity with cross-validated score.\\n\\nParameters\\n\\n----------\\n\\ncv_results : dict of numpy(masked) ndarrays\\n\\nSee attribute cv_results_ of `GridSearchCV`.\\n\\nReturn\\n\\n------\\n\\nint\\n\\nIndex of a model that has the fewest PCA components\\n\\nwhile has its test score within 1 standard deviation of the best\\n\\n`mean_test_score`.\\n\\n\"\"\"\\n\\nthreshold\\n\\nlower_bound\\n\\ncv_results\\n\\ncandidate_idx\\n\\nnp.flatnonzero\\n\\ncv_results\\n\\n\"mean_test_score\"\\n\\n>=\\n\\nthreshold\\n\\nbest_idx\\n\\ncandidate_idx\\n\\ncv_results\\n\\n\"param_reduce_dim__n_components\"\\n\\n][\\n\\ncandidate_idx\\n\\nargmin\\n\\n()\\n\\nreturn\\n\\nbest_idx\\n\\npipe\\n\\nPipeline\\n\\n\"reduce_dim\"\\n\\nPCA\\n\\nrandom_state\\n\\n42\\n\\n)),\\n\\n\"classify\"\\n\\nLinearSVC\\n\\nrandom_state\\n\\n42\\n\\n0.01\\n\\ndual\\n\\n\"auto\"\\n\\n)),\\n\\nparam_grid\\n\\n\"reduce_dim__n_components\"\\n\\n10\\n\\n12\\n\\n14\\n\\n]}\\n\\ngrid\\n\\nGridSearchCV\\n\\npipe\\n\\ncv\\n\\n10\\n\\nn_jobs\\n\\nparam_grid\\n\\nparam_grid\\n\\nscoring\\n\\n\"accuracy\"\\n\\nrefit\\n\\nbest_low_complexity\\n\\nload_digits\\n\\nreturn_X_y\\n\\nTrue\\n\\ngrid\\n\\nfit\\n\\nn_components\\n\\ngrid\\n\\ncv_results_\\n\\n\"param_reduce_dim__n_components\"\\n\\ntest_scores\\n\\ngrid\\n\\ncv_results_\\n\\n\"mean_test_score\"\\n\\nplt.figure\\n\\n()\\n\\nplt.bar\\n\\nn_components\\n\\ntest_scores\\n\\nwidth\\n\\n1.3\\n\\ncolor\\n\\n\"b\"\\n\\nlower\\n\\nlower_bound\\n\\ngrid\\n\\ncv_results_\\n\\nplt.axhline\\n\\nnp.max\\n\\ntest_scores\\n\\n),\\n\\nlinestyle\\n\\n\"--\"\\n\\ncolor\\n\\n\"y\"\\n\\nlabel\\n\\n\"Best score\"\\n\\nplt.axhline\\n\\nlower\\n\\nlinestyle\\n\\n\"--\"\\n\\ncolor\\n\\n\".5\"\\n\\nlabel\\n\\n\"Best score - 1 std\"\\n\\nplt.title\\n\\n\"Balance model complexity and cross-validated score\"\\n\\nplt.xlabel\\n\\n\"Number of PCA components used\"\\n\\nplt.ylabel\\n\\n\"Digit classification accuracy\"\\n\\nplt.xticks\\n\\nn_components\\n\\ntolist\\n\\n())\\n\\nplt.ylim\\n\\n((\\n\\n1.0\\n\\n))\\n\\nplt.legend\\n\\nloc\\n\\n\"upper left\"\\n\\nbest_index_\\n\\ngrid\\n\\nbest_index_\\n\\nprint\\n\\n\"The best_index_ is\\n\\n%d\\n\\nbest_index_\\n\\nprint\\n\\n\"The n_components selected is\\n\\n%d\\n\\nn_components\\n\\nbest_index_\\n\\n])\\n\\nprint\\n\\n\"The corresponding accuracy score is\\n\\n%.2f\\n\\ngrid\\n\\ncv_results_\\n\\n\"mean_test_score\"\\n\\n][\\n\\nbest_index_\\n\\nplt.show\\n\\n()\\n\\nTotal running time of the script: (0 minutes 1.261 seconds)\\n\\nDownload Jupyter notebook: plot_grid_search_refit_callable.ipynb\\n\\nDownload Python source code: plot_grid_search_refit_callable.py\\n\\nRelated examples\\n\\nCustom refit strategy of a grid search with cross-validation\\n\\nCustom refit strategy of a grid search with cross-validation\\n\\nComparing Random Forests and Histogram Gradient Boosting models\\n\\nComparing Random Forests and Histogram Gradient Boosting models\\n\\nRecursive feature elimination with cross-validation\\n\\nRecursive feature elimination with cross-validation\\n\\nSample pipeline for text feature extraction and evaluation\\n\\nSample pipeline for text feature extraction and evaluation\\n\\nPipelining: chaining a PCA and a logistic regression\\n\\nPipelining: chaining a PCA and a logistic regression\\n\\nGallery generated by Sphinx-Gallery\\n\\nShow this page source', start_char_idx=2042, end_char_idx=5155, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "checker('tset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Message(BaseModel):\n",
    "    content: str\n",
    "\n",
    "@app.post(\"/receive_message\")\n",
    "async def receive_message(message: Message):\n",
    "    return {\"message_received\": message.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Youtube_Transcript import y_transcript\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import CodeSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import os\n",
    "os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\"\n",
    "from git import Repo\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from RAG import RAG,RAG_query\n",
    "from vector_storing import vector_storing\n",
    "from vector_load import vector_load\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import VectorStoreIndex,SummaryIndex,StorageContext,SimpleKeywordTableIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "def checker(data):\n",
    "\n",
    "    # response_ext = data\n",
    "    response_ext = {'url':'https://github.com/stanford-futuredata/FrugalGPT','repo_name':'FrugalGPT'}\n",
    "    # response_ext = {'url':'https://docs.sweep.dev/blogs/chunking-2m-files'}\n",
    "\n",
    "\n",
    "    if 'youtube' in response_ext['url'].lower():\n",
    "        # print('hi')\n",
    "        transcript_text = y_transcript(response_ext['url'].split('=')[-1])\n",
    "        if 'video_title' in response_ext:\n",
    "            document = Document(text=transcript_text,\n",
    "                                    metadata = {\"video_title\": response_ext['video_title'], \"video_url\": response_ext['url']})\n",
    "        else:\n",
    "            document = Document(text=transcript_text,\n",
    "                                    metadata = {\"video_url\": response_ext['url']})\n",
    "        documents = [document]\n",
    "        parser = SentenceSplitter(chunk_size=1024)\n",
    "        nodes = parser.get_nodes_from_documents(documents)\n",
    "        RAG(nodes)\n",
    "        # print(documents)\n",
    "\n",
    "    elif 'github' in response_ext['url'].lower():\n",
    "        print(' in github')\n",
    "        repo_name = response_ext['repo_name']\n",
    "        # try:\n",
    "        repo_path = \".\\\\{}\\\\\".format(repo_name)\n",
    "        repo = Repo.clone_from(response_ext['url'], to_path=repo_path)\n",
    "        splitter = CodeSplitter('python')\n",
    "        documents = SimpleDirectoryReader(\"./{}\".format(repo_name),recursive=True,required_exts=[\".py\", \".ipynb\"]).load_data()\n",
    "        for i in documents:\n",
    "            i.metadata['repo_name']=repo_name\n",
    "            i.metadata['repo_link']=response_ext['url']+'/blob/master/'+i.metadata['file_path'][i.metadata['file_path'].find(repo_name)+len(repo_name):]\n",
    "        nodes = splitter.get_nodes_from_documents(documents)\n",
    "        \n",
    "        # code_index = vector_storing(\"./embeddings/code\",\"text-embedding-3-small\",\"vector_store\",nodes,\"SimpleKeywordTableIndex\")\n",
    "        # index = VectorStoreIndex(\n",
    "        #     nodes=nodes\n",
    "        # )\n",
    "        # index.storage_context.persist(persist_dir=\"./new_embeddings/code\")\n",
    "        # storage_context = StorageContext.from_defaults(persist_dir=\"./new_embeddings/code\")\n",
    "        # index = load_index_from_storage(storage_context)\n",
    "        # print(index.index_struct)\n",
    "        code_index = vector_storing(\"./embeddings/code\",\"text-embedding-3-small\",\"vector_store\",nodes,\"VectorStoreIndex\")\n",
    "        code_index = vector_load(\"./embeddings/code\",\"text-embedding-3-small\",\"vector_store\",\"VectorStoreIndex\")\n",
    "            # code_query_engine = code_index.as_query_engine(similarity_top_k=15)\n",
    "\n",
    "            # code_index = vector_load(\"./embeddings/code\",\"text-embedding-3-small\",\"vector_store\",\"SimpleKeywordTableIndex\")\n",
    "            # # print(code_index.index_struct)\n",
    "        code_query_engine = code_index.as_query_engine(similarity_top_k=15)\n",
    "\n",
    "        response = code_query_engine.query('can you list all the models that the author used and give the github repo of the code file where he used all the models')\n",
    "        print(str(response))\n",
    "        print()\n",
    "\n",
    "            # response = code_query_engine.query('Can I get the repo link of scoring code')\n",
    "            # print(str(response))\n",
    "            # # RAG(nodes)\n",
    "        # except:\n",
    "            # splitter = CodeSplitter('python')\n",
    "            # documents = SimpleDirectoryReader(\"./{}\".format(repo_name),recursive=True,required_exts=[\".py\", \".ipynb\"]).load_data()\n",
    "            # for i in documents:\n",
    "            #     i.metadata['repo_name']=repo_name\n",
    "            #     i.metadata['repo_link']=response_ext['url']+'/blob/master/'+i.metadata['file_path'][i.metadata['file_path'].find(repo_name)+len(repo_name):]\n",
    "            # nodes = splitter.get_nodes_from_documents(documents)\n",
    "            \n",
    "            # code_index = vector_storing(\"./embeddings/code\",\"text-embedding-3-small\",\"vector_store\",nodes,\"SimpleKeywordTableIndex\")\n",
    "            # code_query_engine = code_index.as_query_engine(similarity_top_k=15)\n",
    "\n",
    "            # code_index = vector_load(\"./embeddings/code\",\"text-embedding-3-small\",\"vector_store\",\"SimpleKeywordTableIndex\")\n",
    "            # # print(code_index.index_struct)\n",
    "            # code_query_engine = code_index.as_query_engine(similarity_top_k=15)\n",
    "\n",
    "            # response = code_query_engine.query('can you list all the models that the author used and give the github repo of the code file where he used all the models')\n",
    "            # print(str(response))\n",
    "            # print()\n",
    "\n",
    "            # response = code_query_engine.query('Can I get the repo link of scoring code')\n",
    "            # print(str(response))\n",
    "           \n",
    "\n",
    "    else:\n",
    "        loader = UnstructuredURLLoader(urls=[response_ext['url']],  show_progress_bar=True)\n",
    "        documents = loader.load()\n",
    "        parser = SentenceSplitter(chunk_size=1024)\n",
    "        nodes = parser.get_nodes_from_documents(documents)\n",
    "        RAG(nodes)\n",
    "        # print(data)\n",
    "        print(documents[0].metadata)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # print(nodes)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# do webpage text stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in github\n",
      "The author used the following models:\n",
      "1. CohereAIModelProvider\n",
      "2. OpenAIModelProvider\n",
      "3. ForeFrontAIModelProvider\n",
      "4. AnthropicModelProvider\n",
      "5. APIModelProvider\n",
      "6. OpenAIChatModelProvider\n",
      "7. AI21ModelProvider\n",
      "\n",
      "The GitHub repo of the code file where these models were used is: https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\src\\service\\modelservice.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checker('jsdfbjsd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
