{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\"\n",
    "from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language\n",
    "from llama_index.core.node_parser import CodeSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from vector_storing import vector_storing\n",
    "from vector_load import vector_load\n",
    "\n",
    "\n",
    "def github_code(url,repo_name):\n",
    "    repo_path = \".\\\\{}\\\\\".format(repo_name)\n",
    "    repo = Repo.clone_from(url, to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name=\"FrugalGPT\"\n",
    "repo_url = \"https://github.com/stanford-futuredata/FrugalGPT\"\n",
    "try:\n",
    "    github_code(\"https://github.com/stanford-futuredata/FrugalGPT\",repo_name)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CodeSplitter('python')\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./{}\".format(repo_name),recursive=True,required_exts=[\".py\", \".ipynb\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='27d65e76-90ec-480f-9894-880a887672cb', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nget_ipython().run_line_magic(\\'load_ext\\', \\'autoreload\\')\\nget_ipython().run_line_magic(\\'autoreload\\', \\'2\\')\\nimport sys, json, copy \\nimport logging\\nlogging.disable(logging.CRITICAL)\\nsys.path.append(\"src/\")\\n\\n\\n# ## Setup\\n# Next, let us set up the environment. Currently, FrugalGPT leverages LLM APIs from OpenAI (including ChatGPT and GPT-4), AI21, cohere, TextSynth, and Anthropic. Thus we need to set up their API keys. You can still run the notebook without the keys, but API keys are needed if you need FrugalGPT for your own queries.\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4d0c7b91-fb46-4d6d-bb08-71cfc76cecd9', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nimport os\\nos.environ[\\'OPENAI_API_KEY\\'] = \\'OPENAI_API_KEY\\'\\nos.environ[\\'AI21_STUDIO_API_KEY\\'] = \\'AI21_STUDIO_API_KEY\\'\\nos.environ[\\'COHERE_STUDIO_API_KEY\\'] = \\'COHERE_STUDIO_API_KEY\\'\\nos.environ[\\'TEXTSYNTH_API_SECRET_KEY\\'] = \\'TEXTSYNTH_API_SECRET_KEY\\'\\nos.environ[\\'ANTHROPIC_API_KEY\\'] = \\'ANTHROPIC_API_KEY\\'\\nfrom IPython.display import display\\nimport FrugalGPT\\nsupported_LLM = FrugalGPT.getservicename()\\nprint(\"supported LLMs:\",supported_LLM)\\n\\n\\n# ## 1. LLMforAll: One interface for all LLM services\\n# Let us first study an example for LLMforAll, an interface that unifies all existing services.  \\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ac343fa1-5883-4f66-b622-a4c1f7c548ce', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nMyLLMforAll = FrugalGPT.LLMforAll()\\nquery = \"Question: Who is Matei Zaharia in 2023?\\\\nAnswer:\"\\nservice_name = supported_LLM[-1]\\ngenparams = FrugalGPT.GenerationParameter(max_tokens=50, temperature=0.1, stop=[\\'\\\\n\\\\n\\\\n\\\\n\\'])\\nanswer = MyLLMforAll.get_completion(query,service_name,genparams=genparams)\\ncost = MyLLMforAll.get_cost()\\nprint(\"API:\",service_name,\"answer:\",answer,\"cost:\",cost)\\n\\n\\n# The above code snippet shows how to use LLMforAll. Its function get_completion gives a unified inferface for all LLMs: it takes the query, the generation parameters (such as temperature), and the service name as input, and then gives the corresponding generation. The cost can be obtained by calling get_cost(). \\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4cba7aea-3186-4942-a55b-f52f4541cab7', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nresponses = MyLLMforAll.get_completion_allservice(query, supported_LLM, genparams=genparams)\\nprint(\"full responses\")\\ndisplay(responses)\\n\\n\\n# ## 2. LLMCascade: Optimizing performance within budget constraints \\n# Next let us use LLMCascade to automatically optimize the overall performance given a budget constraint.\\n\\n# ### Example usage: predicting gold price trends from financial news\\n# Let us first create a few NLP queries that asks LLM to predict gold price trends.\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='29e07f2f-7c6b-49b1-888e-1f98c217a456', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"\\n\\n\\ndev = [['Q: april gold down 20 cents to settle at $1,116.10/oz\\\\nA:', 'down','0'],\\n       ['Q: gold suffers third straight daily decline\\\\nA:', 'down','1'],\\n       ['Q: Gold futures edge up after two-session decline\\\\nA:', 'up','2'],\\n       ['Q: Dec. gold climbs $9.40, or 0.7%, to settle at $1,356.90/oz\\\\nA:','up','3'],\\n       ['Q: Gold struggles; silver slides, base metals falter\\\\nA:','up','4'],\\n       ['Q: feb. gold ends up $9.60, or 1.1%, at $901.60 an ounce\\\\nA:','up','5'],\\n        ['Q: dent research : is gold\\\\'s day in the sun coming soon?\\\\nA:','none','6']\\n      ]\\nprefix = open('config/prompt/HEADLINES/prefix_e8.txt').read()\\nraw_data = copy.deepcopy(dev)\\ndata = FrugalGPT.formatdata(dev,prefix)\\n\\n\\n# Next let us load a LLMCascade instance.\\n\\n# \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e3a576e0-81a0-4483-8fe4-28bb8b11f862', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nMyCascade = FrugalGPT.LLMCascade()\\nMyCascade.load(loadpath=\"strategy/HEADLINES/\",budget=0.000665)\\n\\n\\n# Let us take a look on LLMCascade\\'s generation on one query.\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a85cffc9-2208-4a69-9bcb-a043d7cd5f9a', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nindex = 2\\nquery = data[index][0]\\nquery_raw = raw_data[index][0]\\ngenparams=FrugalGPT.GenerationParameter(max_tokens=50, temperature=0.1, stop=[\\'\\\\n\\'])\\nanswer = MyCascade.get_completion(query=query,genparams=genparams)\\ncost = MyCascade.get_cost()\\nprint(\"query:\",query_raw)\\nprint(\"FrugalGPT LLMCascade answer:\",answer)\\n\\n\\n# Now we can pass all the queries to both LLMCascade and vanilla GPT-4, and compare their performance.\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52e8d9d5-1c3e-41a0-a864-f099781c3a7c', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# batch generation\\nresult = MyCascade.get_completion_batch(queries=data,genparams=genparams)\\nresult_GPT4 = MyLLMforAll.get_completion_batch(queries=data,genparams=genparams,service_name=\\'openaichat/gpt-4\\')\\nprint(\"FrugalGPT LLMCascade generations\")\\ndisplay(result)\\ndisplay(FrugalGPT.compute_score(result))\\nprint(\"GPT-4 generations\")\\ndisplay(result_GPT4)\\ndisplay(FrugalGPT.compute_score(result_GPT4))\\n\\n\\n# Overall, FrugalGPT LLMCascade gives the same performance but incurs a much smaller cost. This data is of course quite small; Later we will see the evaluation on a larger dataset.\\n\\n# ### Using FrugalGPT-LLMCascade for your own data\\n# Interested in using FrugalGPT for your own data? No problem! The following code snippnet demonstrates how to do it.\\n\\n# The first thing is to load the training dataset.\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd40fe51-ed1f-42bc-8829-15a16e5230aa', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# load data\\ndev = FrugalGPT.loadcsvdata(\"data/HEADLINES/train.csv\")\\ndev = dev[0:10]\\nprefix = open(\\'config/prompt/HEADLINES/prefix_e8.txt\\').read()\\ndata = FrugalGPT.formatdata(dev,prefix)\\n\\n\\n# Second, specify the budget per query, and then train the model. Warning: This can take a while on large datasets!\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4034e4a1-85ab-4748-a02a-99dfa79f2fa5', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"\\n\\n\\n# train the model\\nMyCascade = FrugalGPT.LLMCascade()\\nservice_names = ['openaichat/gpt-3.5-turbo','openaichat/gpt-4','ai21/j1-large','textsynth/gptj_6B']\\nresult = MyCascade.train(data,budget=100,service_names=service_names)\\n\\n\\n# \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d08e805a-39f3-4b0e-bc89-6f210846866c', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# save to disk\\nMyCascade.save(savepath=\"strategy/TEST/\")\\n\\n\\n# Now the model has been saved to disk. You can load it as follows for future applications.\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4feaa28e-c194-44b6-924c-2ffd30a92eb5', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nMyCascade = FrugalGPT.LLMCascade()\\nMyCascade.load(loadpath=\"strategy/TEST/\",budget=100)\\n\\n\\n# ### Performance evaluation\\n# Now let us evaluate the performance of FrugalGPT. We use LLMCascade on the HEADLINES dataset as an example.\\n\\n# First, we load the evaluation dataset and the LLMCascade.\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='597e1d69-e1c1-43ae-9e22-7cb958593fec', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\ntest = FrugalGPT.loadcsvdata(\"data/HEADLINES/test.csv\")\\nprefix = open(\\'config/prompt/HEADLINES/prefix_e8.txt\\').read()\\ndata_eval = FrugalGPT.formatdata(test,prefix)\\nprint(\"test data size:\",len(data_eval))\\nMyCascade = FrugalGPT.LLMCascade()\\nMyCascade.load(loadpath=\"strategy/HEADLINES/\",budget=0.000665)\\n\\n\\n# And then let us evaluate it on the evaluation dataset.\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6e5a1004-6e15-4388-ac57-2a04731ddda4', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb', 'file_name': 'intro.ipynb', 'file_size': 49561, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\ngenparams=FrugalGPT.GenerationParameter(max_tokens=50, temperature=0.1, stop=[\\'\\\\n\\'])\\nresult_GPT4 = MyLLMforAll.get_completion_batch(queries=data_eval,genparams=genparams,service_name=\\'openaichat/gpt-4\\')\\nprint(\"GPT-4 generations\")\\ndisplay(result_GPT4)\\ndisplay(FrugalGPT.compute_score(result_GPT4))\\nresult = MyCascade.get_completion_batch(queries=data_eval,genparams=genparams)\\nprint(\"FrugalGPT LLMCascade generations\")\\ndisplay(result)\\ndisplay(FrugalGPT.compute_score(result))\\n\\n\\n# Overall, LLMCascade achieves better performance than GPT-4 with a 10x smaller cost.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0fbd3437-2bc6-426f-a51a-1b80887db6c3', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\setup.py', 'file_name': 'setup.py', 'file_type': 'text/x-python', 'file_size': 798, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\setup.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"from setuptools import setup, find_packages\\r\\n\\r\\nsetup(\\r\\n    name='FrugalGPT',\\r\\n    version='0.0.1',\\r\\n    author='Lingjiao Chen, Matei Zaharia, and James Zou',\\r\\n    author_email='lingjiao@stanford.edu',\\r\\n    description='The FrugalGPT library',\\r\\n    packages=find_packages(),\\r\\n    install_requires=[\\r\\n        'numpy',\\r\\n        'cohere',\\r\\n        'smart-open',\\r\\n        'jsonlines',\\r\\n        'anthropic==0.2.10',\\r\\n        'scikit-learn',\\r\\n        'evaluate',\\r\\n        'scipy',\\r\\n        'pandas',\\r\\n        'sqlitedict',\\r\\n        'torch',\\r\\n        'transformers',\\r\\n        'accelerate',\\r\\n    ],\\r\\n    classifiers=[\\r\\n        'Programming Language :: Python :: 3',\\r\\n        'License :: OSI Approved :: MIT License',\\r\\n        'Operating System :: OS Independent',\\r\\n    ],\\r\\n    python_requires='>=3.10',\\r\\n)\\r\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6c0ceb57-1823-4992-a254-fbc8f3977fe2', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\__init__.py', 'file_name': '__init__.py', 'file_type': 'text/x-python', 'file_size': 358, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\__init__.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='from .utils import help, getservicename, formatdata, loadcsvdata\\r\\n#from .frugalgpt import FrugalGPT\\r\\nfrom .llmcascade import LLMCascade\\r\\nfrom .llmvanilla import LLMVanilla as LLMforAll\\r\\nfrom .dataloader import DataLoader\\r\\nfrom service.modelservice import GenerationParameter\\r\\nfrom .evaluate import compute_score\\r\\n#from .service.llmengine import LLMEngine\\r\\n\\r\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5827a3d3-7dc4-4414-990d-1a4ac24f0a66', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\dataloader.py', 'file_name': 'dataloader.py', 'file_type': 'text/x-python', 'file_size': 203, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\dataloader.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='class DataLoader(object):\\r\\n    def __init__(self,\\r\\n                 queries,\\r\\n                 prefix=\"\"\\r\\n                 ):\\r\\n        self.queries = queries\\r\\n        self.prefix = prefix\\r\\n        return', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0c997950-0da6-4cb0-998f-17183701661b', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\evaluate.py', 'file_name': 'evaluate.py', 'file_type': 'text/x-python', 'file_size': 701, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\evaluate.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='from service.utils import evaluate\\r\\n\\r\\ndef compute_score(data,metric=\\'em\\'):\\r\\n    \\r\\n    evaluate\\r\\n\\r\\n    score={\"em\":0}\\r\\n    if(metric==\"em\"):\\r\\n        exact_match_percentage = data.apply(calculate_score, axis=1).mean()\\r\\n        score[\\'em\\'] = exact_match_percentage\\r\\n    score[\\'cost\\'] = data[\\'cost\\'].mean()\\r\\n    return score\\r\\n\\r\\n\\r\\n    # Define the scoring function\\r\\ndef calculate_score(row):\\r\\n    # Perform scoring logic based on \\'answer\\' and \\'ref_answer\\'\\r\\n    # For example, you can use a simple equality check and assign a score of 1 for a match, and 0 for a mismatch\\r\\n    score = evaluate(row[\\'answer\\'], row[\\'ref_answer\\'])\\r\\n    #print(\"lens\",len(row[\\'answer\\']),len(row[\\'ref_answer\\']))\\r\\n    return score', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8be2c3bb-8697-46da-b5df-b211e749636e', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\frugalgpt.py', 'file_name': 'frugalgpt.py', 'file_type': 'text/x-python', 'file_size': 118, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\frugalgpt.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='class FrugalGPT(object):\\r\\n    def __init__(self):\\r\\n        # Initialization code for the FrugalGPT class\\r\\n        pass', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91c9576a-e944-48bb-88af-743af423be89', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\llmcache.py', 'file_name': 'llmcache.py', 'file_type': 'text/x-python', 'file_size': 163, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\llmcache.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='class LLMCascade(object):\\r\\n    def __init__(self):\\r\\n        # Initialization code for the FrugalGPT class\\r\\n        return \\r\\n\\r\\n    def load(self):\\r\\n        return\\r\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d7267b90-1cfd-42ca-9678-14704045a249', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\llmcascade.py', 'file_name': 'llmcascade.py', 'file_type': 'text/x-python', 'file_size': 9663, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\llmcascade.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='from .llmvanilla import LLMVanilla\\r\\nfrom service.modelservice import GenerationParameter\\r\\nimport pandas\\r\\nfrom service.utils import evaluate\\r\\nfrom .scoring import Score\\r\\nfrom sklearn.model_selection import train_test_split\\r\\nfrom .llmchain import LLMChain\\r\\nimport json, os\\r\\ndef scorer_text(text):\\r\\n    #return text\\r\\n\\r\\n    newtext = \"Q:\"+text.split(\"Q:\")[-1]    \\r\\n    return newtext\\r\\n\\r\\ndef tempsave(label, response, score,name):\\r\\n    return\\r\\n\\r\\nclass LLMCascade(object):\\r\\n    def __init__(self, \\r\\n                 #service_names =[\\'openaichat/gpt-3.5-turbo\\',\\'openaichat/gpt-4\\'],\\r\\n                 metric=\"em\",\\r\\n                 db_path=\\'db/HEADLINES.sqlite\\',\\r\\n                 ):\\r\\n        # Initialization code for the FrugalGPT class\\r\\n        self.MyLLMEngine = LLMVanilla(db_path=db_path)    \\r\\n        self.MyScores = dict()\\r\\n        self.LLMChain = LLMChain(metric=metric)\\r\\n        #self.service_names =[\\'openaichat/gpt-3.5-turbo\\',\\'openaichat/gpt-4\\'],\\r\\n\\r\\n        return \\r\\n\\r\\n    def load(self,loadpath=\"strategy/HEADLINES/\",budget=0.01):\\r\\n        self.LLMChain = LLMChain()\\r\\n        self.LLMChain.setbudget(budget=budget)\\r\\n        self.LLMChain.loadstrategy(loadpath+\"cascade_strategy.json\")        \\r\\n        model_names = self.loadmodelnames(loadpath)\\r\\n        #print(\"model names\",model_names)\\r\\n        self.scorer = dict()\\r\\n        for name in model_names:\\r\\n            path1 = loadpath+name+\"/\"\\r\\n            self.MyScores[name]=Score()\\r\\n            self.MyScores[name].load(path1)\\r\\n            self.scorer[name]  = self.MyScores[name].get_model()\\r\\n        #print(\"scoer keys:\",self.scorer.keys())\\r\\n        return\\r\\n    \\r\\n    def loadmodelnames(self,loadpath):\\r\\n        directories = []\\r\\n        for entry in os.scandir(loadpath):\\r\\n            if entry.is_dir():\\r\\n                for sub_entry in os.scandir(entry.path):\\r\\n                    if sub_entry.is_dir():\\r\\n                        subdirectory_name = os.path.relpath(sub_entry.path, loadpath)\\r\\n                        directories.append(subdirectory_name)\\r\\n        keys = directories\\r\\n        return keys\\r\\n    def save(self, savepath=\"strategy/HEADLINES/\"):\\r\\n        # Save both Scores and LLChains to the disk\\r\\n        if not os.path.exists(savepath):\\r\\n            os.makedirs(savepath)\\r\\n        self.LLMChain.savestrategy(savepath+\"cascade_strategy.json\")\\r\\n        if(self.no_scorer_train):\\r\\n            return\\r\\n        for name in self.MyScores:\\r\\n            path1 = savepath+name+\"/\"\\r\\n            self.MyScores[name].save(path1)\\r\\n        return\\r\\n    \\r\\n    def train(self,\\r\\n              trainingdata=None,\\r\\n              budget=0.1,\\r\\n              cascade_depth=3,\\r\\n              service_names =[\\'openaichat/gpt-3.5-turbo\\',\\'openaichat/gpt-4\\'],\\r\\n              metric=\"em\",\\r\\n              genparams=GenerationParameter(max_tokens=50, temperature=0.1, stop=[\\'\\\\n\\']),\\r\\n              no_scorer_train=False,\\r\\n              score_type=\\'DistilBert\\',\\r\\n              ):\\r\\n        self.no_scorer_train = no_scorer_train\\r\\n        self.score_type = score_type\\r\\n        # Three major steps\\r\\n        # Step 1: evaluate all services on the given dataset\\r\\n        train, test = train_test_split(trainingdata, test_size=0.01)\\r\\n        #print(\"train and test size\",len(train),len(test))\\r\\n        model_perf_train = self.evaluateall(train,service_names=service_names,metric=metric,genparams=genparams)\\r\\n        model_perf_test = self.evaluateall(test,service_names=service_names,metric=metric,genparams=genparams)\\r\\n        # Step 2: Build the scorer\\r\\n        if(no_scorer_train):\\r\\n            #print(\"directly get the scorers\")\\r\\n            scorers = self.get_scorers()\\r\\n            #print(\"\")\\r\\n        else:\\r\\n            scorers = self.build_scorers(model_perf_train)\\r\\n        # Step 3: Build the cascade\\r\\n        #self.build_cascade(model_perf_test, scorers = scorers, budget=budget, cascade_depth=cascade_depth,metric=metric)\\r\\n        self.build_cascade(model_perf_train, scorers = scorers, budget=budget, cascade_depth=cascade_depth,metric=metric)\\r\\n        return model_perf_test\\r\\n    \\r\\n    def get_completion(self, query,genparams):\\r\\n        LLMChain = self.LLMChain\\r\\n        MyLLMEngine = self.MyLLMEngine\\r\\n        cost = 0 \\r\\n        LLMChain.reset()\\r\\n        while(1):\\r\\n            service_name, score_thres = LLMChain.nextAPIandScore()\\r\\n            if(service_name==None):\\r\\n                break\\r\\n            res = MyLLMEngine.get_completion(query=query,service_name=service_name,genparams=genparams)\\r\\n            cost += MyLLMEngine.get_cost()\\r\\n            score = self.MyScores[service_name].get_score(scorer_text(query+res))\\r\\n            #print(\"score and score thres:\",service_name,score,score_thres)\\r\\n            if(score>1-score_thres):\\r\\n                break\\r\\n        self.cost = cost\\r\\n        return res\\r\\n\\r\\n    def get_completion_batch(self, queries, genparams):\\r\\n        result = list()\\r\\n        for query in queries:\\r\\n            ans1 = self.get_completion(query=query[0], genparams=genparams)\\r\\n            cost = self.get_cost()\\r\\n            result.append({\\'_id\\':query[2],\\'answer\\':ans1,\\'ref_answer\\':query[1],\\'cost\\':cost})\\r\\n        result = pandas.DataFrame(result)\\r\\n        return result\\r\\n        \\r\\n    def get_cost(self):\\r\\n        return self.cost\\r\\n\\r\\n    def _get_response(self,\\r\\n                      data,\\r\\n                      genparams,\\r\\n                      service_name,\\r\\n                      ):\\r\\n        # data is a list\\r\\n        # data[i][0]: query, data[i][1]: answer\\r\\n        result = list()\\r\\n        MyLLMEngine = self.MyLLMEngine\\r\\n        for i in range(len(data)):\\r\\n            query = data[i][0]\\r\\n            temp = dict()\\r\\n            temp[\\'true_answer\\']= data[i][1] \\r\\n            temp[\\'_id\\'] = data[i][2]\\r\\n            temp[\\'query\\'] = query\\r\\n            temp[\\'answer\\'] = MyLLMEngine.get_completion(query=query,service_name=service_name,genparams=genparams)\\r\\n            temp[\\'latency\\'] = MyLLMEngine.get_latency()\\r\\n            temp[\\'cost\\'] = MyLLMEngine.get_cost()\\r\\n            result.append(temp)\\r\\n        return result\\r\\n \\r\\n    def  build_scorers(self,model_perf_train):\\r\\n        self.scorer = dict()\\r\\n        for name in model_perf_train:\\r\\n            self.MyScores[name], self.scorer[name] = self._build_scorer(model_perf_train[name])\\r\\n        return self.scorer\\r\\n\\r\\n    def get_scorers(self):\\r\\n        return self.scorer\\r\\n\\r\\n    def _build_scorer(self,res_and_eval):\\r\\n        #train, test = train_test_split(res_and_eval, test_size=0.2)\\r\\n        #print(\"res_and_eval\",res_and_eval)\\r\\n        traintext = list((res_and_eval[\\'query\\']+res_and_eval[\\'answer\\']).apply(scorer_text))\\r\\n        trainlabel = list(res_and_eval[\\'quality\\'])\\r\\n        MyScore = Score(score_type=self.score_type)\\r\\n        model = MyScore.train(traintext,trainlabel)\\r\\n        return MyScore, model\\r\\n    \\r\\n    def get_scores(self, data, name):\\r\\n        model = self.scorer[name]\\r\\n        scores_dict = dict()\\r\\n        rawdata = data[[\\'_id\\',\\'query\\',\\'answer\\']].to_dict(orient=\\'records\\')\\r\\n        for ptr in rawdata:\\r\\n            text = scorer_text(ptr[\\'query\\']+ptr[\\'answer\\'])\\r\\n            score1 = self.MyScores[name].get_score(text)\\r\\n            scores_dict[ptr[\\'_id\\']] = score1\\r\\n        return scores_dict\\r\\n\\r\\n    def evaluateall(self,train,service_names,metric,genparams):\\r\\n        api_responses = dict()\\r\\n        for name in service_names:\\r\\n            # step 1: get the answers from all API\\r\\n            response = self._get_response(data=train, genparams=genparams,service_name=name)\\r\\n            # step 2: evaluate the performance\\r\\n            res_and_eval = self._evaluate(response, metric=metric)\\r\\n            api_responses[name] = res_and_eval\\r\\n        return api_responses\\r\\n\\r\\n    def _evaluate(self,response, metric=\\'em\\'):\\r\\n        for i in range(len(response)):\\r\\n            ptr = response[i]\\r\\n            score = evaluate(prediction = ptr[\\'answer\\'], ground_truth=ptr[\\'true_answer\\'], metric=metric)\\r\\n            response[i][\\'quality\\'] = score\\r\\n        result = pandas.DataFrame(response)\\r\\n        return result\\r\\n    \\r\\n    def build_cascade(self,model_perf_test, scorers, budget, cascade_depth,metric):\\r\\n        LLMChain1 = LLMChain(metric=metric,L_max=cascade_depth)\\r\\n        LLMChain1.setbudget(budget=budget)\\r\\n        responses = dict()\\r\\n        scores = dict()\\r\\n        for key in model_perf_test:\\r\\n            labels = model_perf_test[key][[\\'_id\\',\\'true_answer\\']].rename(columns={\\'true_answer\\': \\'answer\\'}).to_dict(orient=\\'records\\')\\r\\n            responses[key] = table2json(model_perf_test[key])\\r\\n            scores[key] = self.get_scores(model_perf_test[key],name=key)\\r\\n            tempsave(labels,responses[key],scores[key],key)\\r\\n        #print(\"responses\",responses)  \\r\\n                  \\r\\n        LLMChain1.train(responses,labels,scores)\\r\\n        self.LLMChain = LLMChain1\\r\\n        return\\r\\n    \\r\\ndef table2json(df):\\r\\n    # Convert DataFrame to the desired dictionary format\\r\\n    result_dict = {}\\r\\n    for _, row in df.iterrows():\\r\\n        answer = row[\\'answer\\']\\r\\n        cost = row[\\'cost\\']\\r\\n        _id = row[\\'_id\\']\\r\\n        quality = row[\\'quality\\']\\r\\n    \\r\\n        # Update \"answer\" key\\r\\n        if \\'answer\\' not in result_dict:\\r\\n            result_dict[\\'answer\\'] = dict()\\r\\n        result_dict[\\'answer\\'][_id]= answer\\r\\n    \\r\\n        # Update \"cost\" key\\r\\n        if \\'cost\\' not in result_dict:\\r\\n            result_dict[\\'cost\\'] = dict()\\r\\n        result_dict[\\'cost\\'][_id]= cost\\r\\n    \\r\\n        # Update \"quality\" key\\r\\n        if \\'quality\\' not in result_dict:\\r\\n            result_dict[\\'quality\\'] = dict()\\r\\n        result_dict[\\'quality\\'][_id]= quality\\r\\n\\r\\n    result_dict[\\'sp\\'] = dict()\\r\\n    result_dict[\\'logprobs\\'] = dict()\\r\\n    return result_dict\\r\\n\\r\\nclass strategy():\\r\\n    def __init__(self):\\r\\n        return\\r\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1a606cf1-27d8-4472-94c4-e1bbfbc5169f', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\llmchain.py', 'file_name': 'llmchain.py', 'file_type': 'text/x-python', 'file_size': 7528, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\llmchain.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# -*- coding: utf-8 -*-\\r\\n\"\"\"\\r\\nCreated on Sat Apr  8 15:10:54 2023\\r\\n\\r\\n@author: CLJ\\r\\n\"\"\"\\r\\nimport os, copy, numpy, itertools, logging, json\\r\\nfrom . import optimizer\\r\\nimport service.utils as utils\\r\\n\\r\\nclass Strategy(object):\\r\\n    def __init__(self,metric=\"\"):\\r\\n        self.metric=metric\\r\\n        self.budget=100\\r\\n        return\\r\\n    def setbudget(self,budget=10,):\\r\\n        self.budget = budget\\r\\n    def train(self,responses:dict,labels:dict):\\r\\n        raise NotImplementedError\\r\\n    def evaluate(self,responses:dict,labels:dict):\\r\\n        return utils.evaluate_batch(responses,labels)            \\r\\n    def loadstrategy(self,strategy_path=\"\"):\\r\\n        if(1<2):\\r\\n            filepath = self.strategy_path\\r\\n            if(strategy_path==\"\"):\\r\\n                filepath = self.strategy_path\\r\\n            else:\\r\\n                filepath = strategy_path\\r\\n                self.strategy_path = strategy_path\\r\\n\\r\\n            budget = self.budget\\r\\n            logging.critical(\"loading strategy from:{}\".format(filepath))\\r\\n            strategy = json.load(open(filepath))    \\r\\n            data = strategy[\\'budget\\'][str(budget)]\\r\\n            self.thres = data[\\'thres_list\\']\\r\\n            self.model_ids = data[\\'model_list\\']\\r\\n            self.quantile = data[\\'quantile\\']\\r\\n            return True\\r\\n\\r\\n        try:\\r\\n            filepath = self.strategy_path\\r\\n            if(strategy_path==\"\"):\\r\\n                filepath = self.strategy_path\\r\\n            else:\\r\\n                filepath = strategy_path\\r\\n                self.strategy_path = strategy_path\\r\\n\\r\\n            budget = self.budget\\r\\n            logging.critical(\"loading strategy from:{}\".format(filepath))\\r\\n            strategy = json.load(open(filepath))    \\r\\n            data = strategy[\\'budget\\'][str(budget)]\\r\\n            self.thres = data[\\'thres_list\\']\\r\\n            self.model_ids = data[\\'model_list\\']\\r\\n            self.quantile = data[\\'quantile\\']\\r\\n            return True\\r\\n        except:\\r\\n            print(\"fail to load\")\\r\\n            return False\\r\\n        \\r\\n    def savestrategy(self,strategy_path=\"\"):\\r\\n        filepath = self.strategy_path\\r\\n        if(strategy_path==\"\"):\\r\\n            filepath = self.strategy_path\\r\\n        else:\\r\\n            filepath = strategy_path\\r\\n            self.strategy_path = strategy_path\\r\\n        thres_list = self.thres\\r\\n        model_list = self.model_ids\\r\\n        budget = self.budget\\r\\n        isExist = os.path.exists(filepath)\\r\\n        if(isExist):\\r\\n            strategy = json.load(open(filepath))     \\r\\n        else:\\r\\n            strategy = dict()\\r\\n            strategy[\\'budget\\'] = dict()\\r\\n        data = dict()\\r\\n        data[\\'thres_list\\'] = list(thres_list)\\r\\n        data[\\'model_list\\'] = list(model_list)\\r\\n        data[\\'quantile\\'] = list(self.quantile)\\r\\n        strategy[\\'budget\\'][str(budget)] = data\\r\\n        #print(\"the strategy is\",strategy)\\r\\n        json_object = json.dumps(strategy, indent=4)\\r\\n        # Writing to sample.json\\r\\n        with open(filepath, \"w\") as outfile:\\r\\n            outfile.write(json_object)\\r\\n        return\\r\\n    \\r\\nclass SingleAPI(Strategy):\\r\\n    \\r\\n    def train(self,responses:dict,labels:dict):\\r\\n        return \\r\\n    def predict(self,):\\r\\n        return \\r\\n    \\r\\n    \\r\\nclass LLMChain(Strategy):\\r\\n    def __init__(self,\\r\\n                 metric=\"em_mc\",\\r\\n                 L_max=2,\\r\\n                 strategy_path=\"strategy/temp2.json\",):\\r\\n        self.metric=metric    \\r\\n        self.model_ids=[\"CHATGPT\",\"GPT-4\"]\\r\\n        self.thres = [-1]\\r\\n        self.strategy_path = strategy_path\\r\\n        self.L_max = L_max\\r\\n        \\r\\n    def train(self,responses:dict,labels:dict,score:dict):\\r\\n        # load strategy\\r\\n        \\'\\'\\'\\r\\n        if(self.loadstrategy()):\\r\\n            print(\"loading strategy successfully!\")\\r\\n            return \\r\\n        \\'\\'\\'\\r\\n        # train contains two stages.\\r\\n        #print(\"start training\")\\r\\n        # 1. Enumerate all LLM chain and compute the results\\r\\n        L_max = self.L_max\\r\\n        service_ids = responses.keys()\\r\\n        results = []\\r\\n        for ell in range(L_max,L_max+1):\\r\\n            # fix the choice of all apis in the chain\\r\\n            selected_ids = list(itertools.permutations(service_ids, ell))\\r\\n            # get results\\r\\n            results += [ self._find_param(responses,labels,selected_id,score) for selected_id in selected_ids]\\r\\n\\r\\n        # 2. Pick the one with the best results\\r\\n        acc = -1\\r\\n        model_ids = []\\r\\n        thres = []\\r\\n        quantile = []\\t\\t\\r\\n        for result in results:\\r\\n            if(result[\\'acc\\']>acc):\\r\\n                acc = result[\\'acc\\']\\r\\n                model_ids = result[\\'model_ids\\']\\r\\n                thres = result[\\'thres\\']\\r\\n                quantile = result[\\'quantile\\']\\r\\n        self.model_ids = model_ids\\r\\n        self.thres = thres\\r\\n        self.quantile = quantile\\r\\n        #print(\"finish training!\")\\r\\n        # 3 save \\r\\n        self.savestrategy()\\r\\n        return \\r\\n    \\r\\n    def _find_param(self,\\r\\n                    responses,\\r\\n                    labels,\\r\\n                    selected_id,\\r\\n\\t\\t\\t\\t\\tscores):\\r\\n        # construct data\\r\\n        L_mat, C_mat, d_mat = optimizer.construct_data(responses,\\r\\n                                                       labels,\\r\\n                                                       selected_id,\\r\\n                                                       metric=self.metric,\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   scores=scores,\\r\\n                                                       )\\r\\n        # optimze results\\r\\n        obj, var, qual = optimizer.optimize(L_mat,C_mat,d_mat,budget=self.budget)\\r\\n        result = {\"acc\":obj,\"model_ids\":selected_id,\"thres\":var,\"quantile\":qual}\\r\\n        return result\\r\\n    \\r\\n    def predict(self, responses:dict,scores:dict):\\r\\n        base_id = self.model_ids[0]\\r\\n        result = copy.deepcopy(responses[base_id])\\r\\n        result[\\'query_apis\\'] = dict()\\r\\n        \\r\\n        dist_full = [optimizer.compute_distance_batch(responses,self.model_ids[0:i+1],scores=scores) for i in range(len(self.model_ids))]\\r\\n        i = 0\\r\\n        for key in responses[base_id][\\'answer\\']:\\r\\n            data1 = [responses[m_id][\\'answer\\'][key] for m_id in self.model_ids]\\r\\n            cost1 = [responses[m_id][\\'cost\\'][key] for m_id in self.model_ids]\\r\\n            dist_1 = [dist_full[j][i] for j in range(len(self.model_ids))]\\r\\n            i+=1\\r\\n            answer, cost, apis = self.predict_one(data1, cost1,dist_1)\\r\\n            result[\\'answer\\'][key] = answer\\r\\n            result[\\'cost\\'][key] = cost\\r\\n            result[\\'query_apis\\'][key] = apis\\r\\n        return result\\r\\n    \\r\\n    def predict_one(self, data1, cost1,dist_1):\\r\\n        #full_cost = cost1[0]\\r\\n        #apis=[self.model_ids[0]]\\r\\n        apis = []\\r\\n        full_cost= 0\\r\\n        for i in range(0,len(self.model_ids)-1):\\r\\n            #dist = optimizer.compute_dist(data1[0:i+1])\\r\\n            dist = dist_1[i]\\r\\n            full_cost += cost1[i]\\r\\n            apis.append(self.model_ids[i])\\r\\n            if(dist<self.thres[i]):\\r\\n                return data1[i], full_cost, apis\\r\\n        full_cost += cost1[-1]\\r\\n        apis.append(self.model_ids[-1])\\r\\n        return data1[-1], full_cost, apis\\r\\n    \\r\\n    \\r\\n    def show(self):\\r\\n        print(\"chain models\", self.model_ids)\\r\\n    def getAPInames(self):\\r\\n        return self.model_ids\\r\\n\\r\\n    def reset(self):\\r\\n        self.APIptr = 0\\r\\n\\r\\n    def nextAPIandScore(self,):\\r\\n        if (self.APIptr>=len(self.model_ids)):\\r\\n            return None, None\\r\\n        name = self.model_ids[self.APIptr]\\r\\n        scorethres = self.thres[self.APIptr]\\r\\n        self.APIptr+=1\\r\\n        return name, scorethres\\r\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='53b91cf5-8b24-4a92-9ee6-d13d73e465d2', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\llmvanilla.py', 'file_name': 'llmvanilla.py', 'file_type': 'text/x-python', 'file_size': 7649, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\llmvanilla.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='from service.modelservice import make_model,GenerationParameter\\r\\nfrom sqlitedict import SqliteDict\\r\\nimport json, logging, pandas\\r\\nfrom transformers import GPT2Tokenizer\\r\\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\r\\nfrom .utils import help, getservicename\\r\\n\\r\\nglobal mydict\\r\\n\\r\\nserviceidmap = json.load(open(\"config/serviceidmap.json\"))\\r\\n\\r\\ndef form_keys(service_id,\\r\\n              genparams,\\r\\n              query,\\r\\n              ):\\r\\n    mydict1 = genparams.get_dict()\\r\\n    mydict1[\\'service_id\\'] = service_id\\r\\n    mydict1[\\'query\\'] = query\\r\\n    return repr(mydict1)\\r\\n    \\r\\nclass LLMVanilla(object):\\r\\n    def __init__(self, \\r\\n                 service_name=None,\\r\\n                 db_path=\"db/qa_cache.sqlite\",\\r\\n                 db_path_new = \\'db/HEADLINES.sqlite\\',\\r\\n                 ):\\r\\n        if(service_name==None):\\r\\n            service_name = getservicename()\\r\\n        global mydict\\r\\n        mydict = SqliteDict(db_path, autocommit=True)\\r\\n        global newdict\\r\\n        newdict = SqliteDict(db_path_new, autocommit=True)\\r\\n        self.service_name = service_name\\r\\n        self.services = dict()\\r\\n        self.cost = 0\\r\\n        for item in service_name:\\r\\n            provider = item.split(\\'/\\')[0]\\r\\n            name = item.split(\\'/\\')[1]\\r\\n            self.services[item] = make_model(provider,name)\\r\\n        return\\r\\n    \\r\\n    def get_cost(self,):\\r\\n        return self.cost\\r\\n    \\r\\n    def compute_cost(self,\\r\\n                     input_text,\\r\\n                     output_text,\\r\\n                     service_name):\\r\\n        cost = 0\\r\\n        input_size = len(tokenizer(input_text)[\\'input_ids\\'])\\r\\n        gen_size = len(tokenizer(output_text)[\\'input_ids\\']) \\r\\n        \\r\\n        service_all = self.service_name\\r\\n    \\r\\n        \\r\\n        cost_output = service_all[service_name][\"cost_output\"]\\r\\n        cost_input = service_all[service_name][\"cost_input\"]\\r\\n        cost_fixed = service_all[service_name][\"cost_fixed\"]\\r\\n        fixed_size = service_all[service_name][\"fixed_size\"]\\r\\n        \\r\\n        cost = cost_input*input_size + cost_fixed\\r\\n        if(gen_size>fixed_size):\\r\\n            cost += cost_output*(gen_size - fixed_size)    \\r\\n        \\r\\n        return cost \\r\\n\\r\\n    def costestimate(self,\\r\\n                     query:str,\\r\\n                     service_name = \"20000\",\\r\\n                     Params = GenerationParameter(max_tokens=50, \\r\\n                                                   temperature=0.1,\\r\\n                                                   stop=[\"\\\\n\"],\\r\\n                      )\\r\\n                     ):\\r\\n        max_token = Params.max_tokens\\r\\n        cost = self.compute_cost(input_text = query,\\r\\n                                 output_text = \\'token \\'*max_token,\\r\\n                                 service_name=service_name,) \\r\\n        \\r\\n        return cost\\r\\n\\r\\n    \\r\\n    def get_completion(self, \\r\\n                      query:str,\\r\\n                      service_name = \"20000\",\\r\\n                      use_save=False,\\r\\n                      use_db = True,\\r\\n                      savepath=\"raw.pkl\",\\r\\n                      genparams = GenerationParameter(max_tokens=50, \\r\\n                                                   temperature=0.1,\\r\\n                                                   stop=[\"\\\\n\"],\\r\\n),\\r\\n                      migrate_db=False,\\r\\n                      ):\\r\\n        model = self.services[service_name]\\r\\n        key = service_name+\"_\"+query\\r\\n        #logging.critical(\"Service name is{}\".format(service_name))\\r\\n\\r\\n        if(service_name in serviceidmap):\\r\\n            key = form_keys(service_id=serviceidmap[service_name], genparams=genparams, query=query)\\r\\n        else:\\r\\n            key = form_keys(service_id=service_name, genparams=genparams, query=query)\\r\\n        if((key in mydict )and (\\'cost\\' in mydict[key]) ):\\r\\n            #logging.critical(\"Use cached results! for key{}\".format(key))\\r\\n\\r\\n            completion = mydict[key]\\r\\n            newdict[key] = mydict[key]\\r\\n        else:\\r\\n            print(\"---invoke new API call---\")\\r\\n            print(service_name)\\r\\n            #print(query)\\r\\n            completion = model.getcompletion(query,\\r\\n                                         use_save=use_save,\\r\\n                                         genparams=genparams)  \\r\\n            mydict[key] = completion\\r\\n     \\r\\n        if(use_db==False):\\r\\n            completion = model.getcompletion(query,\\r\\n                                         use_save=use_save,\\r\\n                                         genparams=genparams)\\r\\n            mydict[key] = completion\\r\\n            #print(\"do not use stored results!\")\\r\\n        try:\\r\\n            cost =completion[\\'cost\\']\\r\\n        except:\\r\\n            cost = self.compute_cost(input_text = query,\\r\\n                                 output_text = completion[\\'completion\\'],\\r\\n                                 service_name=service_name,)    \\r\\n        self.cost = cost\\r\\n        self.completion = completion \\r\\n        if(\\'latency\\' not in completion):\\r\\n            self.latency = \\'NA\\'\\r\\n        else:\\r\\n            self.latency = completion[\\'latency\\']    \\r\\n        return completion[\\'completion\\']\\r\\n\\r\\n    def get_completion_allservice(self, \\r\\n                      query:str,\\r\\n                      service_names = [\\'\\'],\\r\\n                      use_save=False,\\r\\n                      use_db = True,\\r\\n                      savepath=\"raw.pkl\",\\r\\n                      genparams = GenerationParameter(max_tokens=50, \\r\\n                                                   temperature=0.1,\\r\\n                                                   stop=[\"\\\\n\"],\\r\\n),\\r\\n                      ):\\r\\n        result = list()\\r\\n        for name in service_names:\\r\\n            answer = self.get_completion(query=query,\\r\\n                                         service_name=name,\\r\\n                                         use_save=use_save,\\r\\n                                         use_db=use_db,\\r\\n                                         savepath=savepath,\\r\\n                                         genparams=genparams,\\r\\n                                         )    \\r\\n            cost = self.get_cost()\\r\\n            result.append({\\'service\\':name,\\'answer\\':answer,\\'cost\\':cost})\\r\\n        result = pandas.DataFrame(result)\\r\\n        return result\\r\\n    \\r\\n    def get_completion_batch(self, \\r\\n                      queries:[],\\r\\n                      service_name = \\'\\',\\r\\n                      use_save=False,\\r\\n                      use_db = True,\\r\\n                      savepath=\"raw.pkl\",\\r\\n                      genparams = GenerationParameter(max_tokens=50, \\r\\n                                                   temperature=0.1,\\r\\n                                                   stop=[\"\\\\n\"],\\r\\n),\\r\\n                      ):\\r\\n        result = list()\\r\\n        for query in queries:\\r\\n            answer = self.get_completion(query=query[0],\\r\\n                                         service_name=service_name,\\r\\n                                         use_save=use_save,\\r\\n                                         use_db=use_db,\\r\\n                                         savepath=savepath,\\r\\n                                         genparams=genparams,\\r\\n                                         )    \\r\\n            cost = self.get_cost()\\r\\n            result.append({\\'_id\\':query[2],\\'answer\\':answer,\\'ref_answer\\':query[1],\\'cost\\':cost})\\r\\n        result = pandas.DataFrame(result)\\r\\n        return result\\r\\n    \\r\\n    def get_last_cost(self,\\r\\n                      ):\\r\\n        #print(\"completion with cost\",self.completion)\\r\\n        return self.completion[\\'cost\\']\\r\\n    \\r\\n    def get_latency(self):\\r\\n        return self.latency\\r\\n\\r\\n    def reset(self,\\r\\n              ):\\r\\n        self.cost = 0\\r\\n        self.completion = None\\r\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='47c6b0cf-48c5-4658-9dac-1d3fe77b0daf', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\optimizer.py', 'file_name': 'optimizer.py', 'file_type': 'text/x-python', 'file_size': 5062, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\optimizer.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='import numpy, logging, scipy\\r\\nimport service.utils as utils\\r\\n#from scipy import optimize\\r\\n\\r\\neps = 1e-3\\r\\n\\r\\n\\r\\ndef compute_dist(answers, scores, query=None):\\r\\n    dist = 1-scores[-1]\\r\\n    #print(\"scores is:\",scores)\\r\\n    return dist\\r\\n\\r\\ndef compute_distance_batch(responses,selected_id,scores):\\r\\n    numpy.random.seed(2023)\\r\\n    #print(\"scores are\",scores[\\'GPT-4\\'][\\'8606\\'])\\r\\n    answer_list = [ [responses[id1][\\'answer\\'][key] for id1 in selected_id ] for key in responses[selected_id[0]][\\'answer\\']]\\r\\n    score_list = [ [scores[id1][key] for id1 in selected_id ] for key in responses[selected_id[0]][\\'answer\\']]\\r\\n    dists = [compute_dist(answer_list[i],scores=score_list[i]) for i in range(len(answer_list))]\\r\\n    return dists\\r\\n    \\r\\n\\r\\ndef compute_loss_and_cost_batch(response,labels,metric):\\r\\n    results_eval = utils.evaluate_batch(response,labels)\\r\\n    costs = results_eval[\\'cost_list\\']\\r\\n    loss = results_eval[metric+\"_list\"]\\r\\n    return loss, costs\\r\\n\\r\\ndef construct_data(responses,\\r\\n                   labels,\\r\\n                   selected_id,\\r\\n                   metric,\\r\\n\\t\\t\\t\\t   scores,\\r\\n                   ):\\r\\n    # L: n by d matrix, loss\\r\\n    # C: n by d matrix, cost up to chain pos\\r\\n    # d: n by d matrix, distance up to chain pos\\r\\n    n = len(labels)\\r\\n    d = len(selected_id)\\r\\n    L_mat = numpy.zeros((n,d))\\r\\n    C_mat = numpy.zeros((n,d))\\r\\n    d_mat = numpy.zeros((n,d))\\r\\n    # compute L\\r\\n    for idx, id1 in enumerate(selected_id):\\r\\n        L_mat[:,idx], C_mat[:,idx] = compute_loss_and_cost_batch(responses[id1],labels,metric)\\r\\n    # compute C\\r\\n    for i in range(1,len(selected_id)):\\r\\n        C_mat[:,i]+=C_mat[:,i-1]\\r\\n    # compute d\\r\\n    for idx, id1 in enumerate(selected_id):\\r\\n        d_mat[:,idx] = compute_distance_batch(responses,selected_id[0:idx+1],scores=scores)\\r\\n    \\r\\n    logging.debug(\"Showing the average (acc) of the L_mat\")\\r\\n    logging.debug(numpy.average(L_mat,0))\\r\\n    logging.critical(\"construct data for {}\".format(selected_id))\\r\\n    #logging.critical(\"SHOW Distance mat\",d_mat[0:20,])\\r\\n    return L_mat,C_mat,d_mat\\r\\n\\r\\ndef optimize(L_mat,C_mat,d_mat,budget):\\r\\n    #return 0.1, [0,1,1,1]\\r\\n    if(numpy.average(C_mat[:,0])>budget):\\r\\n        logging.critical(\"Base API too expensive, skip\")\\r\\n        return -999, [], []\\r\\n    mask_full = numpy.full(L_mat.shape[0],True)\\r\\n    def f(delta):\\r\\n        #delta.append(1.0)\\r\\n        #delta_0 = numpy.append(delta,1)\\r\\n        e_full = (d_mat < delta)\\r\\n        #print(\"shape of e_full\",e_full.shape)\\r\\n        for i in range(e_full.shape[0]):\\r\\n            has_accept = False\\r\\n            for j in range(e_full.shape[1]):\\r\\n                if(has_accept):\\r\\n                    e_full[i,j] = False\\r\\n                else:\\r\\n                    if(e_full[i,j]==1):\\r\\n                        has_accept=True\\r\\n            \\r\\n        #print(\"e_full sum:\",numpy.sum(e_full,1))\\r\\n        #delta.pop()       \\r\\n        acc = numpy.sum(numpy.multiply(e_full,L_mat))       \\r\\n        cost = numpy.sum(numpy.multiply(e_full,C_mat))\\r\\n        if(cost>budget*len(L_mat)):\\r\\n            return 10000\\r\\n        return -acc\\r\\n\\r\\n    def g(qual):\\r\\n        if(numpy.all(numpy.diff(qual) >= 0)==False):\\r\\n            return 10000\\r\\n        #logging.info(\"qual value is {}\".format(qual))\\r\\n        thres = quatile2thres_batch(qual)\\r\\n        #thres = [-0.1,-0.1,0.2,1]\\r\\n        logging.info(\"qual and thres value are:{} and {}\".format(qual, thres))        \\r\\n        return f(thres)\\r\\n\\r\\n    def quatile2thres(q,i,mask_last):\\r\\n        data = d_mat[mask_last,i]\\r\\n        logging.debug(\"data is\",i, data)\\r\\n        q = max(min(q,1),0)\\r\\n        thres1 = numpy.quantile(data, 1-q)\\r\\n        masknew = d_mat[:,i]>=thres1\\r\\n        masknew = numpy.logical_and(mask_last,masknew)\\r\\n        return thres1, masknew\\r\\n\\r\\n    def quatile2thres_batch(qual):\\r\\n        thres = numpy.zeros(L_mat.shape[1])\\r\\n        thres[-1] = 1\\r\\n        mask_last = mask_full\\r\\n        for i in range(0,len(thres)-1):\\r\\n            # map quatile to the thres\\r\\n            qi = qual[i]\\r\\n            thres[i],mask_last = quatile2thres(qi,i,mask_last)\\r\\n        return thres\\r\\n    #delta_ranges = [(-0.1,1.01)]*(L_mat.shape[1]-1)\\r\\n    #delta_ranges[-1] = (1.0,1.0)\\r\\n    #d1 = [-0.1, -0.1, 0.05,1]\\r\\n    #print(\"Test of the function f:\",f(d1)/len(L_mat),d1)\\r\\n    #print(\"delta range\",delta_ranges, L_mat.shape)\\r\\n    qual_ranges = [(1e-5,1-1e-5)]*(L_mat.shape[1]-1)\\r\\n#    qual_ranges = [[0.1,0.5,0.95]]*(L_mat.shape[1]-1)\\r\\n    logging.debug(\"start searching\")\\r\\n    \\r\\n\\r\\n#    optimize.brute()\\r\\n    resbrute = scipy.optimize.brute(g, \\r\\n                              qual_ranges, \\r\\n                              #args=params, \\r\\n                              full_output=True,\\r\\n                              finish=scipy.optimize.fmin,\\r\\n                              #workers=2,\\r\\n                              Ns=40,\\r\\n                              )\\r\\n    logging.critical(\"the obj is {} and the var is {}\".format(resbrute[1],resbrute[0]))\\r\\n    #time.sleep(10000)\\r\\n    thres_final = quatile2thres_batch(resbrute[0])\\r\\n    return -resbrute[1]/len(L_mat), thres_final, resbrute[0]\\r\\n        ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='467668e6-dd0d-44ee-87b6-a9749fc4b115', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\scoring.py', 'file_name': 'scoring.py', 'file_type': 'text/x-python', 'file_size': 10458, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\scoring.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='import evaluate, json, numpy, torch\\r\\nfrom sklearn.model_selection import train_test_split\\r\\nfrom transformers import DistilBertTokenizerFast, BertTokenizerFast, AlbertTokenizerFast, AutoModelForSequenceClassification\\r\\nfrom transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, BertForSequenceClassification, AlbertForSequenceClassification\\r\\nfrom transformers import GPT2ForSequenceClassification, XLNetForSequenceClassification, XLNetTokenizer\\r\\nfrom torch.nn import functional as F\\r\\n\\r\\nfrom transformers import set_seed\\r\\n\\r\\nfrom transformers import GPT2Tokenizer\\r\\nimport re\\r\\n\\r\\nset_seed(2023)\\r\\n\\r\\ntext_form=\"em_mc\"\\r\\ntext_form=\"em\"\\r\\n\\r\\n\\r\\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\\r\\n#print(\"device is:\",device)\\r\\n\\r\\n#tokenizer = DistilBertTokenizerFast.from_pretrained(\\'distilbert-base-uncased\\')\\r\\n\\r\\n# tokenizer = DistilBertTokenizerFast.from_pretrained(\\'distilbert-base-uncased\\')\\r\\n\\r\\n#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\r\\n#tokenizer.pad_token = tokenizer.eos_token\\r\\n\\r\\n#tokenizer = XLNetTokenizer.from_pretrained(\\'xlnet-base-cased\\')\\r\\n\\r\\nmetric = evaluate.load(\"accuracy\")\\r\\n\\r\\ndef compute_metrics(eval_pred):\\r\\n    logits, labels = eval_pred\\r\\n    predictions = numpy.argmax(logits, axis=-1)\\r\\n    return metric.compute(predictions=predictions, references=labels)\\r\\n\\r\\n\\r\\ndef save_jsonline(data,savepath):\\r\\n    print(\"data is\",data)\\r\\n    json_object = json.dumps(str(data), indent=4)\\r\\n    with open(savepath,\\'w\\') as f:\\r\\n        f.write(json_object)    \\r\\n    return \\r\\n\\r\\ndef load_data(q_path=\"test/1000_perf_questions.json\",\\r\\n        a_path=\"test/1000_perf_answers.json\",\\r\\n        label_path=\\'test/1000_perf_em_list.txt\\',):\\r\\n  print(\"label path\",label_path)\\r\\n  q1 = json.load(open(q_path))\\r\\n  a1 = json.load(open(a_path))\\r\\n  labels = numpy.loadtxt(label_path,dtype=int)\\r\\n  text = list()\\r\\n  for item in q1:\\r\\n    id1 = item[\\'_id\\']\\r\\n    q_cur = item[\\'query\\'].split(\"\\\\n\\\\n\")[-1]\\r\\n    ans1 = a1[\\'answer\\'][id1]\\r\\n    if(text_form==\"em_mc\"):\\r\\n        ans1=ans2opt(ans1)\\r\\n    query = q_cur+\" \"+ans1\\r\\n    text.append(query)\\r\\n  return text, labels\\r\\n  \\r\\ndef ans2opt(ans1):\\r\\n    def mc_remove(text):\\r\\n        a1 = re.findall(\\'\\\\([a-zA-Z]\\\\)\\', text)\\r\\n        #print(\"text is\",text)\\r\\n        #print(\"a1\",a1)\\r\\n        if(len(a1)==0):\\r\\n            return \"\"\\r\\n        return re.findall(\\'\\\\([a-zA-Z]\\\\)\\', text)[-1]\\r\\n    ans2 = mc_remove(ans1)\\r\\n    return ans2  \\r\\n\\r\\nclass IMDbDataset(torch.utils.data.Dataset):\\r\\n    def __init__(self, encodings, labels):\\r\\n        self.encodings = encodings\\r\\n        self.labels = labels\\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\r\\n        item[\\'labels\\'] = torch.tensor(self.labels[idx])\\r\\n        return item\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.labels)\\r\\n\\r\\n\\r\\nclass Score(object):\\r\\n    def __init__(self,          \\r\\n                 score_type=\\'DistilBert\\'):\\r\\n        if(score_type==\\'DistilBert\\'):\\r\\n            self.tokenizer = DistilBertTokenizerFast.from_pretrained(\\'distilbert-base-uncased\\')\\r\\n        if(score_type==\\'Bert\\'):\\r\\n            self.tokenizer = BertTokenizerFast.from_pretrained(\\'bert-base-uncased\\')\\r\\n        if(score_type==\\'AlBert\\'):\\r\\n            self.tokenizer = AlbertTokenizerFast.from_pretrained(\\'albert-base-v2\\')\\r\\n        self.score_type = score_type\\r\\n        return\\r\\n    def train(self,\\r\\n              train_texts, \\r\\n              train_labels,\\r\\n              #score_type=\\'DistilBert\\',\\r\\n              ):\\r\\n        \\r\\n        train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.6)\\r\\n        #print(\"train_text 0\",train_texts[0])\\r\\n        #print(\"val_text 0\",val_texts[0])\\r\\n        #print(\"----------------------------\")\\r\\n        tokenizer = self.tokenizer\\r\\n        train_encodings = tokenizer(train_texts, truncation=True, padding=True,max_length=512)\\r\\n        val_encodings = tokenizer(val_texts, truncation=True, padding=True,max_length=512)\\r\\n\\r\\n        train_dataset = IMDbDataset(train_encodings, train_labels)\\r\\n        val_dataset = IMDbDataset(val_encodings, val_labels)\\r\\n\\r\\n        training_args = TrainingArguments(\\r\\n    output_dir=\\'./scorer_location\\',          # output directory\\r\\n    num_train_epochs=8,              # total number of training epochs\\r\\n    per_device_train_batch_size=16,  # batch size per device during training\\r\\n    per_device_eval_batch_size=64,   # batch size for evaluation\\r\\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\\r\\n    weight_decay=0.01,               # strength of weight decay\\r\\n    logging_dir=\\'./logs\\',            # directory for storing logs\\r\\n    logging_steps=10,\\r\\n    evaluation_strategy=\"epoch\",\\r\\n\\tsave_strategy =\"epoch\",\\r\\n    load_best_model_at_end=True,\\r\\n    seed=2023,\\r\\n    )\\r\\n        score_type = self.score_type\\r\\n        if(score_type==\\'DistilBert\\'):\\r\\n            model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\\r\\n        if(score_type==\\'Bert\\'):\\r\\n            model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\\r\\n        if(score_type==\\'AlBert\\'):\\r\\n            model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\")\\r\\n\\r\\n        #model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\")\\r\\n        #model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\")\\r\\n\\r\\n        model = model.to(device)\\r\\n        trainer = Trainer(\\r\\n    model=model,                         # the instantiated 🤗 Transformers model to be trained\\r\\n    args=training_args,                  # training arguments, defined above\\r\\n    train_dataset=train_dataset,         # training dataset\\r\\n    eval_dataset=val_dataset ,            # evaluation dataset\\r\\n    compute_metrics=compute_metrics,\\r\\n\\r\\n        )\\r\\n\\r\\n        trainer.train()\\r\\n        self.trainer = trainer\\r\\n        self.model = model\\r\\n        return model\\r\\n    \\r\\n    def predict(self,\\r\\n                model,text):\\r\\n        #trainer = self.trainer\\r\\n        model = self.model\\r\\n        encoding = self.tokenizer(text, return_tensors=\"pt\",truncation=True, padding=True)\\r\\n        encoding = {k: v.to(model.device) for k,v in encoding.items()}\\r\\n        outputs = model(**encoding)\\r\\n        logit_score = outputs.logits.cpu().detach()\\r\\n        #return logit_score\\r\\n        # convert logit score to torch array\\r\\n        torch_logits = logit_score\\r\\n        \\r\\n        # get probabilities using softmax from logit score and convert it to numpy array\\r\\n        probabilities_scores = F.softmax(torch_logits, dim = -1).numpy()[0]\\r\\n\\r\\n        return probabilities_scores\\r\\n    \\r\\n    def get_model(self):\\r\\n        return self.model\\r\\n\\r\\n    def get_score(self,text):\\r\\n        prob = self.predict(\"\",text)\\r\\n        return prob[1]\\r\\n\\r\\n    def gen_score(self,\\r\\n                  model,\\r\\n                  texts,\\r\\n                  ):\\r\\n        scores = list()\\r\\n        for text in texts:\\r\\n            prob = self.predict(model,text)\\r\\n            scores.append(prob[1])\\r\\n        return scores\\r\\n    def save(self,savepath):\\r\\n        self.trainer.save_model(savepath)\\r\\n        return\\r\\n\\r\\n    def load(self,loadpath):\\r\\n        self.model = AutoModelForSequenceClassification.from_pretrained(loadpath)\\r\\n        return\\r\\n    def save_scores(self,\\r\\n                    q_path,\\r\\n                    score_path,\\r\\n                    scores):\\r\\n        q1 = json.load(open(q_path))\\r\\n        result = dict()\\r\\n        i = 0\\r\\n        for item in q1:\\r\\n            id1 = item[\\'_id\\']\\r\\n            result[id1] = scores[i]\\r\\n            i+=1\\r\\n        save_jsonline(data=result, savepath=score_path)    \\r\\n        return\\r\\n    def pipelines(self,\\r\\n                  train_q_path,\\r\\n                  train_a_path,\\r\\n                  train_label_path,\\r\\n                  train_score_path,\\r\\n                  \\r\\n                  val_q_path,\\r\\n                  val_a_path,\\r\\n                  val_label_path,\\r\\n                  val_score_path,\\r\\n                  \\r\\n                  test_q_path,\\r\\n                  test_a_path,\\r\\n                  test_label_path,\\r\\n                  test_score_path,\\r\\n                  ):\\r\\n        # generate data\\r\\n        train_texts, train_labels = load_data(\\r\\n                q_path=train_q_path,\\r\\n                a_path=train_a_path,\\r\\n                label_path=train_label_path)\\r\\n\\r\\n        test_texts, test_labels = load_data(\\r\\n                q_path=test_q_path,\\r\\n                a_path=test_a_path,\\r\\n                label_path=test_label_path)\\r\\n\\r\\n        val_texts, val_labels = load_data(\\r\\n                q_path=val_q_path,\\r\\n                a_path=val_a_path,\\r\\n                label_path=val_label_path)\\r\\n        \\r\\n        # train the model\\r\\n        model = self.train(train_texts, train_labels)\\r\\n        \\r\\n        # get scores \\r\\n        scores = self.gen_score(model,test_texts)\\r\\n        self.save_scores(test_q_path,test_score_path,scores)\\r\\n        scores = self.gen_score(model,val_texts)\\r\\n        self.save_scores(val_q_path,val_score_path,scores)\\r\\n        scores = self.gen_score(model,train_texts)\\r\\n        self.save_scores(train_q_path,train_score_path,scores)\\r\\n        return\\r\\n    \\r\\ndef main():\\r\\n    print(\"test of scoring functions\")\\r\\n    MyScore = Score()\\r\\n    train_q_path=\"../../api_performance/headlines/9_train/1000_perf_questions.json\"\\r\\n    train_a_path=\"../../api_performance/headlines/9_train/1000_perf_answers.json\"\\r\\n    train_label_path=\"../../api_performance/headlines/9_train/1000_perf_em_list.txt\"\\r\\n    train_score_path=\"../../api_performance/headlines/9_train/1000_perf_scores.json\"\\r\\n\\r\\n    val_q_path=\"../../api_performance/headlines/9_train/1000_perf_questions.json\"\\r\\n    val_a_path=\"../../api_performance/headlines/9_train/1000_perf_answers.json\"\\r\\n    val_label_path=\"../../api_performance/headlines/9_train/1000_perf_em_list.txt\"\\r\\n    val_score_path=\"../../api_performance/headlines/9_train/1000_perf_scores.json\"\\r\\n                  \\r\\n    test_q_path=\"../../api_performance/headlines/9/1000_perf_questions.json\"\\r\\n    test_a_path=\"../../api_performance/headlines/9/1000_perf_answers.json\"\\r\\n    test_label_path=\"../../api_performance/headlines/9/1000_perf_em_list.txt\"\\r\\n    test_score_path=\"../../api_performance/headlines/9/1000_perf_scores.json\"\\r\\n                  \\r\\n    MyScore.pipelines(train_q_path, train_a_path, train_label_path, train_score_path, val_q_path, val_a_path, val_label_path, val_score_path, test_q_path, test_a_path, test_label_path, test_score_path)\\r\\n    return    \\r\\nif __name__ == \"__main__\":\\r\\n   main()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='842384d2-0f76-4346-905c-6574c8ef5a4c', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\FrugalGPT\\\\utils.py', 'file_name': 'utils.py', 'file_type': 'text/x-python', 'file_size': 936, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\utils.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='import json\\r\\nimport csv\\r\\n\\r\\ndef help():\\r\\n    print(\"Welcome to use FrugalGPT!\")\\r\\n    print(\"FrugalGPT currently support the following methods\")\\r\\n    print(\"LLMCascade, and LLMforAll!\")\\r\\n    return \\r\\n\\r\\ndef getservicename(configpath=\\'config/serviceinfo.json\\'):\\r\\n    service = json.load(open(configpath))\\r\\n    names = [provider + \"/\" + name for provider in service.keys() for name in service[provider]]\\r\\n    return names\\r\\n\\r\\ndef formatdata(data, prefix):\\r\\n    for i in range(len(data)):\\r\\n        data[i][0]=prefix+data[i][0]\\r\\n    return data\\r\\n\\r\\ndef loadcsvdata(filename):\\r\\n    # Initialize an empty list to store the data\\r\\n    data = []\\r\\n\\r\\n    # Open the file in read mode\\r\\n    with open(filename, \"r\") as csvfile:\\r\\n        reader = csv.reader(csvfile)\\r\\n        # Iterate over each row in the CSV file\\r\\n        for row in reader:\\r\\n            # Append the row as a list to the data list\\r\\n            data.append(row)\\r\\n    return data        ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f17d0f27-c5ed-4361-b4e7-4150c70c1d1f', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\service\\\\__init__.py', 'file_name': '__init__.py', 'file_type': 'text/x-python', 'file_size': 29, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\service\\\\__init__.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='#from .utils import evaluate#', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='11a32109-4d04-4969-b5c2-92315112e988', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\service\\\\modelservice.py', 'file_name': 'modelservice.py', 'file_type': 'text/x-python', 'file_size': 19728, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\service\\\\modelservice.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='import requests\\r\\nimport os\\r\\nimport time\\r\\nimport pickle\\r\\nimport cohere\\r\\nimport json\\r\\nfrom .utils import compute_cost\\r\\nimport os\\r\\nimport anthropic\\r\\n#from transformers import CodeGenTokenizerFast\\r\\n#tokenizer_FFAI = CodeGenTokenizerFast.from_pretrained(\"Salesforce/codegen-350M-mono\")\\r\\nfrom transformers import GPT2Tokenizer\\r\\n\\r\\ntokenizer_FFAI = GPT2Tokenizer.from_pretrained(\"gpt2\")\\r\\n\\r\\nclass GenerationParameter(object):\\r\\n    def __init__(self,\\r\\n                 max_tokens=100,\\r\\n                 temperature=0.1,\\r\\n                 stop=[\"\\\\n\"],\\r\\n                 date=\"20230301\",\\r\\n                 trial = 0,\\r\\n                 ):\\r\\n        self.max_tokens=max_tokens\\r\\n        self.temperature=temperature\\r\\n        self.stop = stop\\r\\n        self.date = date\\r\\n        self.readable = dict()\\r\\n        self.readable[\\'max_tokens\\'] = max_tokens        \\r\\n        self.readable[\\'temperature\\'] = temperature\\r\\n        self.readable[\\'stop\\'] = stop\\r\\n        self.readable[\\'date\\'] = date\\r\\n        #self.readable[\\'trial\\'] = trial\\r\\n\\r\\n\\r\\n    def get_dict(self,\\r\\n                 ):\\r\\n        return self.readable        \\r\\n                  \\r\\nclass ModelService:\\r\\n    \"\"\"Base class for a model provider, currently we support API models, but this class can be subclassed with support\\r\\n    for local GPU models\"\"\"\\r\\n    def getcompletion(self,\\r\\n                      context,\\r\\n                      use_save=False,\\r\\n                      savepath=\"raw.pkl\",\\r\\n                      genparams = GenerationParameter(),\\r\\n                      ):\\r\\n        \"\"\"\\r\\n            Given a context, generate a text output\\r\\n        \"\"\"        \\r\\n        raise NotImplementedError\\r\\n\\r\\nclass APIModelProvider(ModelService):\\r\\n    \"\"\"Provider that calculates conditional logprobs through a REST API\"\"\"\\r\\n    _CONFIG = json.load(open(\"config/serviceinfo.json\"))\\r\\n    def getcompletion(self,\\r\\n                      context,\\r\\n                      use_save=False,\\r\\n                      savepath=\"raw.pkl\",\\r\\n                      genparams = GenerationParameter(),\\r\\n                      ):\\r\\n        endpoint = self._get_endpoint()\\r\\n        req = self._request_format(context,\\r\\n                                   genparams)\\r\\n        self.context = context\\r\\n        if(use_save==True):\\r\\n            #print(\"-try previous call-\",savepath)\\r\\n            try:\\r\\n                response = self.read_response(savepath)\\r\\n                #print(\\'Have predicted before.\\')\\r\\n                #print(\\'prediction\\',self.mlservice.read_response(finalpath))\\r\\n            except:\\r\\n                #print(\"new call\")\\r\\n                response = self._api_call( endpoint, data=req, api_key=self._API_KEY, retries=10, retry_grace_time=10)\\r\\n                self.write_response(savepath)\\r\\n        else:\\r\\n            time1 = time.time()\\r\\n            response = self._api_call( endpoint, data=req, api_key=self._API_KEY, retries=10, retry_grace_time=10)\\r\\n            latency = time.time()-time1\\r\\n\\r\\n        result = self._response_format(response)\\r\\n        cost = self._get_cost(context,result)        \\r\\n        result[\\'cost\\'] = cost\\r\\n        result[\\'latency\\'] = latency\\r\\n\\r\\n        return result\\r\\n\\r\\n    def read_response(self, path=\"test\"):\\r\\n        f = open(path, \\'rb\\')\\r\\n        self.response = pickle.load(f)\\r\\n        #self._response2pred(self.response)\\r\\n        f.close()\\r\\n        try:\\r\\n            return self.response.json()\\r\\n        except:\\r\\n            return self.response\\r\\n            \\r\\n    def write_response(self,path=\"test\"):\\r\\n        filehandler = open(path, \\'wb\\') \\r\\n        pickle.dump(self.response, filehandler)\\r\\n        filehandler.close()\\r\\n        try:\\r\\n            return self.response.json()\\r\\n        except:\\r\\n            return self.response\\r\\n        \\r\\n    def _request_format(self,\\r\\n                     genparams):\\r\\n        raise NotImplementedError\\r\\n\\r\\n    def _response_format(self,\\r\\n                         response,\\r\\n                         ):\\r\\n        raise NotImplementedError\\r\\n         \\r\\n    def _api_call(self, endpoint, data, api_key, retries=10, retry_grace_time=10):\\r\\n        for i in range(retries):\\r\\n            #print(\"data is\",data)\\r\\n            #\\'\\'\\'\\r\\n            \\'\\'\\'\\r\\n            res = requests.post(\\r\\n                endpoint,\\r\\n                headers={\"Authorization\": f\"Bearer {api_key}\"},\\r\\n                json=data,\\r\\n                timeout=60)\\r\\n            print(\"res is\",res)\\r\\n            print(\"made it\")\\r\\n            \\'\\'\\'\\r\\n            while True:\\r\\n                try:\\r\\n                    res = requests.post(\\r\\n                endpoint,\\r\\n                headers={\"Authorization\": f\"Bearer {api_key}\"},\\r\\n                json=data,\\r\\n                timeout=60)\\r\\n                    res.raise_for_status()  # Raise an exception if the response is an HTTP error\\r\\n                    break  # If we got a response, exit the loop\\r\\n                except (requests.exceptions.Timeout, requests.exceptions.RequestException):\\r\\n                    # If we got a timeout or another kind of error, retry the request\\r\\n                    print(\"timeout, retry\")\\r\\n                    continue\\r\\n            #\\'\\'\\'\\r\\n            #print(\"res is\", res)\\r\\n            if res.status_code == 200:\\r\\n                #print(\"succeed!\")\\r\\n                self.response = res\\r\\n                return res.json()\\r\\n            else:\\r\\n                print(\"failed res is:\",res)\\r\\n            print(f\"API call failed with {res}. Waiting {retry_grace_time} seconds\")\\r\\n            time.sleep(retry_grace_time)\\r\\n        raise TimeoutError(f\"API request failed {retries} times, giving up!\") \\r\\n\\r\\n    def _get_cost(self,\\r\\n                  context,\\r\\n                  completion,\\r\\n                ):\\r\\n        tk1, tk2 = self._get_io_tokens(context,completion)\\r\\n        cost = compute_cost(tk1,tk2,self._CONFIG[self._NAME][self._model])\\r\\n        return cost\\r\\n    \\r\\n    def _get_endpoint(self):\\r\\n        endpoint = self._ENDPOINT.format(engine=self._model)\\r\\n        return endpoint\\r\\n        \\r\\nclass OpenAIModelProvider(APIModelProvider):\\r\\n    _ENDPOINT = os.environ.get(\"OPENAI_ENDPOINT\", \"https://api.openai.com/v1/engines/{engine}/completions\")\\r\\n    _API_KEY = os.environ.get(\\'OPENAI_API_KEY\\', None)\\r\\n    _NAME = \"openai\"\\r\\n    \\r\\n    def __init__(self, model):\\r\\n        self._model = model\\r\\n        self.temp_cmp = {\\'raw\\':{\\'usage\\':{\\'prompt_tokens\\':0}}}\\r\\n        assert self._API_KEY is not None, \"Please set OPENAI_API_KEY env var for running through OpenAI\"\\r\\n\\r\\n    def _request_format(self,\\r\\n                        context,\\r\\n                        genparams):\\r\\n        #tk1,tk2 = self._get_io_tokens(context,self.temp_cmp)\\r\\n        if(self._model in [\\'tƒd\\']):\\r\\n            #tk1 = len(context) # TODO: change this to real token count\\r\\n            tk1 = len(tokenizer_FFAI(context)[\\'input_ids\\'])\\r\\n            print(\"length:\",tk1)\\r\\n            if(tk1+genparams.max_tokens>=2047):\\r\\n                print(\"warning: the length is too large. only take the last few ones\")\\r\\n                context = context[-2048+genparams.max_tokens:]\\r\\n        req = {\\r\\n            \"prompt\": context,\\r\\n            \"echo\": False,\\r\\n            \"max_tokens\": genparams.max_tokens,\\r\\n            \"logprobs\": 1,\\r\\n            \"temperature\": genparams.temperature,\\r\\n            \"top_p\": 1,\\r\\n            \"stop\":genparams.stop,\\r\\n        }\\r\\n        return req\\r\\n    \\r\\n    def _response_format(self,\\r\\n                         response,\\r\\n                         ):\\r\\n        result = dict()\\r\\n        result[\\'raw\\'] = response\\r\\n        result[\"completion\"] = response[\\'choices\\'][0][\\'text\\']\\r\\n        return result    \\r\\n    \\r\\n    def _get_io_tokens(self,context, completion):\\r\\n        #print(\"completion raw:\",completion[\\'raw\\'])\\r\\n        tk1 = completion[\\'raw\\'][\\'usage\\'][\\'prompt_tokens\\']\\r\\n        try:\\r\\n            tk2 = completion[\\'raw\\'][\\'usage\\'][\\'completion_tokens\\']\\r\\n        except:\\r\\n            tk2 = 0\\r\\n            \\r\\n        return tk1, tk2  \\r\\n    \\r\\nclass OpenAIChatModelProvider(APIModelProvider):\\r\\n    _ENDPOINT = os.environ.get(\"OPENAICHAT_ENDPOINT\", \"https://api.openai.com/v1/chat/completions\")\\r\\n    _API_KEY = os.environ.get(\\'OPENAI_API_KEY\\', None)\\r\\n    _NAME = \"openaichat\"\\r\\n    \\r\\n    def __init__(self, model):\\r\\n        self._model = model\\r\\n        assert self._API_KEY is not None, \"Please set OPENAI_API_KEY env var for running through OpenAI\"\\r\\n\\r\\n    def _request_format(self,\\r\\n                        context,\\r\\n                        genparams):\\r\\n        req = {\\r\\n            \"messages\": [{\"content\":context,\"role\":\"user\"}],\\r\\n            #\"echo\": False,\\r\\n            \"max_tokens\": genparams.max_tokens,\\r\\n            #\"logprobs\": 1,\\r\\n            \"temperature\": genparams.temperature,\\r\\n            #\"top_p\": 1,\\r\\n            #\"stop\":, # seems it does not allow \\\\n to be the stopping token.\\r\\n            \"model\":self._model,\\r\\n            #\"role\":\"user\",\\r\\n        }\\r\\n        #print(\"the request is---\")\\r\\n        #print(req)\\r\\n        return req\\r\\n    \\r\\n    def _response_format(self,\\r\\n                         response,\\r\\n                         ):\\r\\n        #print(\"response is\",response)\\r\\n        result = dict()\\r\\n        result[\\'raw\\'] = response\\r\\n        result[\"completion\"] = response[\\'choices\\'][0][\\'message\\'][\\'content\\']\\r\\n        return result    \\r\\n    \\r\\n    def _get_io_tokens(self,context, completion):\\r\\n        #print(\"completion raw:\",completion[\\'raw\\'])\\r\\n        tk1 = completion[\\'raw\\'][\\'usage\\'][\\'prompt_tokens\\']\\r\\n        try:\\r\\n            tk2 = completion[\\'raw\\'][\\'usage\\'][\\'completion_tokens\\']\\r\\n        except:\\r\\n            tk2 = 0\\r\\n            \\r\\n        return tk1, tk2  \\r\\n\\r\\nclass AI21ModelProvider(APIModelProvider):\\r\\n    _ENDPOINT = os.environ.get(\"AI21_STUDIO_ENDPOINT\", \"https://api.ai21.com/studio/v1/{engine}/complete\")\\r\\n    _API_KEY = os.environ.get(\\'AI21_STUDIO_API_KEY\\', None)\\r\\n    _NAME = \"ai21\"\\r\\n    def __init__(self, model):\\r\\n        self._model = model\\r\\n        assert self._API_KEY is not None, \"Please set AI21_STUDIO_API_KEY env var for running through AI21 Studio\"\\r\\n\\r\\n    def _request_format(self,\\r\\n                        context,\\r\\n                        genparams):\\r\\n        req = {\\r\\n            \"prompt\": context,\\r\\n            \"maxTokens\": genparams.max_tokens,\\r\\n            \"temperature\": genparams.temperature,\\r\\n            \"stopSequences\":genparams.stop,\\r\\n        }           \\r\\n        return req\\r\\n    \\r\\n    def _response_format(self,\\r\\n                         response,\\r\\n                         ):\\r\\n        result = dict()\\r\\n        result[\\'raw\\'] = response\\r\\n        result[\"completion\"] = response[\\'completions\\'][0][\\'data\\'][\\'text\\']\\r\\n        return result    \\r\\n    \\r\\n    def _get_io_tokens(self,context, completion):\\r\\n        tk1 = len(completion[\\'raw\\'][\\'prompt\\'][\\'tokens\\'])\\r\\n        tk2 = len(completion[\\'raw\\'][\\'completions\\'][0][\\'data\\'][\\'tokens\\'])\\r\\n        return tk1, tk2 \\r\\n\\r\\nclass CohereAIModelProvider(APIModelProvider):\\r\\n    _ENDPOINT = os.environ.get(\"COHERE_STUDIO_ENDPOINT\", \"https://api.ai21.com/studio/v1/{engine}/complete\")\\r\\n    _API_KEY = os.environ.get(\\'COHERE_STUDIO_API_KEY\\', None)\\r\\n    _NAME = \"cohere\"\\r\\n\\r\\n    def __init__(self, model):\\r\\n        self._model = model\\r\\n        assert self._API_KEY is not None, \"Please set COHERE_STUDIO_API_KEY env var for running through AI21 Studio\"\\r\\n    def _api_call(self, endpoint, data, api_key, retries=10, retry_grace_time=10):\\r\\n        \\r\\n        co = cohere.Client(api_key)\\r\\n        try:\\r\\n            response = co.generate( \\r\\n            model=self._model, \\r\\n            prompt=data[\\'prompt\\'], \\r\\n            max_tokens=data[\\'max_tokens\\'], \\r\\n            temperature=data[\\'temperature\\'], \\r\\n            k=data[\\'k\\'], \\r\\n            p=data[\\'p\\'], \\r\\n            frequency_penalty=data[\\'frequency_penalty\\'], \\r\\n            presence_penalty=data[\\'presence_penalty\\'], \\r\\n            stop_sequences=data[\\'stop_sequences\\'], \\r\\n            return_likelihoods=data[\\'return_likelihoods\\']) \\r\\n        except:\\r\\n            response = co.generate( \\r\\n            model=self._model, \\r\\n            prompt=\"test\", \\r\\n            max_tokens=data[\\'max_tokens\\'], \\r\\n            temperature=data[\\'temperature\\'], \\r\\n            k=data[\\'k\\'], \\r\\n            p=data[\\'p\\'], \\r\\n            frequency_penalty=data[\\'frequency_penalty\\'], \\r\\n            presence_penalty=data[\\'presence_penalty\\'], \\r\\n            stop_sequences=data[\\'stop_sequences\\'], \\r\\n            return_likelihoods=data[\\'return_likelihoods\\']) \\r\\n            \\r\\n        self.response = response\\r\\n        return response\\r\\n\\r\\n    def _request_format(self,\\r\\n                        context,\\r\\n                        genparams):\\r\\n        req = {\\r\\n            \"prompt\": context,\\r\\n            \"model\":self._model,\\r\\n            #\"topKReturn\": 10,\\r\\n             \"max_tokens\":genparams.max_tokens, \\r\\n             \"temperature\":genparams.temperature, \\r\\n            \"k\":1,\\r\\n            \"num_generations\":1,\\r\\n            \"p\":1, \\r\\n            \"frequency_penalty\":0, \\r\\n            \"presence_penalty\":0, \\r\\n            \"stop_sequences\":genparams.stop, \\r\\n            \"return_likelihoods\":\\'ALL\\',          \\r\\n        }\\r\\n        return req\\r\\n    \\r\\n    def _response_format(self,\\r\\n                         response,\\r\\n                         ):\\r\\n        fullresponse = dict()\\r\\n        fullresponse[\\'raw\\'] = \\'\\'\\r\\n\\r\\n        try:\\r\\n            #print(\"response is:\", response)\\r\\n            #print(\"type of the response\",type(response))\\r\\n            token_likelihoods = response.generations[0].token_likelihoods\\r\\n            text = [i1.token for i1 in token_likelihoods]\\r\\n            text = \"\".join(text)\\r\\n            #print(\"text is\",text[len(context):])\\r\\n            fullresponse[\"completion\"] = response.generations[0].text\\r\\n        except:\\r\\n            fullresponse[\"completion\"]=\\'\\'\\r\\n        #print(\"completion:\",fullresponse[\"completion\"])\\r\\n        return fullresponse\\r\\n    \\r\\n    def _get_io_tokens(self,context, completion):\\r\\n        tk1 = len(context)/1000\\r\\n        tk2 = len(completion)/1000\\r\\n        return tk1, tk2\\r\\n\\t\\r\\nclass ForeFrontAIModelProvider(APIModelProvider):\\r\\n    _API_KEY = os.environ.get(\\'FOREFRONT_API_KEY\\', None)  \\r\\n    _NAME = \"ffai\"\\r\\n    _ENDPOINT_MAP = {\"QA\":\"https://shared-api.forefront.link/organization/nKKlZP3F37RN/codegen-16b-nl/completions/eGQdyiZlHIW4\",\\r\\n                     \"CodeGen\":\"https://shared-api.forefront.link/organization/nKKlZP3F37RN/codegen-16b-nl/completions/eGQdyiZlHIW4\",\\r\\n                     \"Pythia\":\"https://shared-api.forefront.link/organization/nKKlZP3F37RN/pythia-20b/completions/vanilla\",\\r\\n                     }\\r\\n    def __init__(self, model=\"QA\"):\\r\\n        self._model = model\\r\\n        assert self._API_KEY is not None, \"Please set FOREFRONT_API_KEY env var for running through Forefront\"  \\r\\n    \\r\\n    def _request_format(self,\\r\\n                        context,\\r\\n                        genparams):\\r\\n#        if(self._model in [\\'QA\\',\\'text-babbage-001\\',\\'text-ada-001\\']):\\r\\n        if(1):\\r\\n            #tk1 = len(context) # TODO: change this to real token count\\r\\n            tk1 = len(tokenizer_FFAI(context)[\\'input_ids\\'])\\r\\n            print(\"length:\",tk1)\\r\\n            if(tk1+genparams.max_tokens>=2047):\\r\\n                print(\"warning: the length is too large. only take the last few ones\")\\r\\n                context = context[-2048+genparams.max_tokens:]\\r\\n        \\r\\n        req = {\\r\\n            \"text\": context,\\r\\n            \"numResults\": 1,\\r\\n            \"length\": genparams.max_tokens,\\r\\n            \"topKReturn\": 1,\\r\\n            \"temperature\":genparams.temperature,\\r\\n            \"stop\":[\"\\\\n\"],\\r\\n            \"logprobs\":1,\\r\\n            \"echo\":False,\\r\\n        }\\r\\n        return req\\r\\n    \\r\\n    def _response_format(self,\\r\\n                         response,\\r\\n                         ):\\r\\n        result = dict()\\r\\n        result[\"raw\"] = response\\r\\n        result[\\'completion\\'] = response[\\'result\\'][0][\\'completion\\']\\r\\n        return result    \\r\\n    \\r\\n    def _get_io_tokens(self,context, completion):\\r\\n        tk1 = len(tokenizer_FFAI(context)[\\'input_ids\\'])\\r\\n        tk2 = len(completion[\\'raw\\'][\"logprobs\"][\\'tokens\\'])\\r\\n        return tk1, tk2\\r\\n    \\r\\n    def _get_endpoint(self):\\r\\n        endpoint = self._ENDPOINT_MAP[self._model]\\r\\n        return endpoint\\r\\n\\t\\t\\t  \\r\\nclass TextSynthModelProvider(APIModelProvider):\\r\\n    _ENDPOINT = os.environ.get(\"TEXTSYNTH_ENDPOINT\", \"https://api.textsynth.com/v1/engines/{engine}/completions\")\\r\\n    _API_KEY = os.environ.get(\\'TEXTSYNTH_API_SECRET_KEY\\', None)\\r\\n    _NAME = \"textsynth\"\\r\\n\\r\\n    def __init__(self, model=\"QA\"):\\r\\n        self._model = model\\r\\n        assert self._API_KEY is not None, \"Please set FOREFRONT_API_KEY env var for running through Forefront\"\\r\\n\\t\\t\\r\\n    def _request_format(self,\\r\\n                        context,\\r\\n                        genparams):\\r\\n        req = {\\r\\n            \"prompt\": context,\\r\\n            \"max_tokens\": genparams.max_tokens,\\r\\n            \"temperature\":genparams.temperature,\\r\\n            \"stop\":genparams.stop,\\r\\n        }\\r\\n        return req\\r\\n    \\r\\n    def _response_format(self,\\r\\n                         response,\\r\\n                         ):\\r\\n        result = dict()\\r\\n        result[\\'raw\\'] = response\\r\\n        result[\\'completion\\'] = result[\\'raw\\'][\\'text\\']\\r\\n        return result    \\r\\n    \\r\\n    def _get_io_tokens(self,context, completion):\\r\\n        tk1 = completion[\\'raw\\'][\\'input_tokens\\']\\r\\n        tk2 = completion[\\'raw\\'][\\'output_tokens\\']\\r\\n        return tk1, tk2\\r\\n\\r\\nclass AnthropicModelProvider(APIModelProvider):\\r\\n    _ENDPOINT = os.environ.get(\"ANTHROPIC_ENDPOINT\", \"https://api.anthropic.com/v1/complete\")\\r\\n    _API_KEY = os.environ.get(\\'ANTHROPIC_API_KEY\\', None)\\r\\n    _NAME = \"anthropic\"\\r\\n    \\r\\n    def __init__(self, model):\\r\\n        self._model = model\\r\\n        assert self._API_KEY is not None, \"Please set ANTHROPIC_API_KEY env var for running through OpenAI\"\\r\\n        self.client = anthropic.Client(os.environ[\\'ANTHROPIC_API_KEY\\'])\\r\\n        #self.client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\\r\\n\\r\\n    def _request_format(self,\\r\\n                        context,\\r\\n                        genparams):\\r\\n        req = {\\r\\n            \"prompt\": context,\\r\\n            #\"echo\": False,\\r\\n            \"max_tokens_to_sample\": genparams.max_tokens,\\r\\n            #\"logprobs\": 1,\\r\\n            #\"temperature\": genparams.temperature,\\r\\n            #\"top_p\": 1,\\r\\n            #\"stop\":, # seems it does not allow \\\\n to be the stopping token.\\r\\n            \"model\":self._model,\\r\\n            #\"role\":\"user\",\\r\\n        }\\r\\n        #print(\"the request is---\")\\r\\n        #print(req)\\r\\n        return req\\r\\n    \\r\\n    def _response_format(self,\\r\\n                         response,\\r\\n                         ):\\r\\n        #print(\"response is\",response)\\r\\n        result = dict()\\r\\n        result[\\'raw\\'] = response\\r\\n        result[\"completion\"] = response[\\'completion\\']\\r\\n        return result    \\r\\n    \\r\\n    def _get_io_tokens(self,context, completion):\\r\\n        #print(\"completion raw:\",completion[\\'raw\\'])\\r\\n        tk1 = anthropic.count_tokens(context)\\r\\n        tk2 = anthropic.count_tokens(completion[\\'completion\\'])\\r\\n        return tk1, tk2\\r\\n\\r\\n    def _api_call(self, endpoint, data, api_key, retries=10, retry_grace_time=10):\\r\\n        response = self.client.completion(\\r\\n            prompt=anthropic.HUMAN_PROMPT+data[\\'prompt\\']+anthropic.AI_PROMPT,\\r\\n            model=data[\\'model\\'],\\r\\n            max_tokens_to_sample=data[\\'max_tokens_to_sample\\'],\\r\\n            )  \\r\\n        return response      \\r\\n\\r\\n_PROVIDER_MAP = {\\r\\n    \"openai\": OpenAIModelProvider, # cleaned\\r\\n    \"ai21\": AI21ModelProvider, # cleaned\\r\\n    \"cohere\":CohereAIModelProvider, # cleaned\\r\\n    \"forefrontai\":ForeFrontAIModelProvider, # cleaned\\r\\n    \"textsynth\":TextSynthModelProvider, # cleaned\\r\\n    \"openaichat\":OpenAIChatModelProvider,# cleaned\\r\\n    \"anthropic\":AnthropicModelProvider,\\r\\n}\\r\\n\\r\\n\\r\\ndef make_model(provider, model):\\r\\n    assert provider in _PROVIDER_MAP, f\"No model provider \\'{provider}\\' implemented\"\\r\\n    return _PROVIDER_MAP[provider](model)\\r\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='12069ec2-d3fb-47b7-a7cc-2b84daf1e1a8', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\service\\\\modelservice_test.py', 'file_name': 'modelservice_test.py', 'file_type': 'text/x-python', 'file_size': 1289, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\service\\\\modelservice_test.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='from modelservice import make_model\\r\\nimport os, requests, numpy, argparse\\r\\n\\r\\ndef main(): \\r\\n    parser = argparse.ArgumentParser(description=\\'LLM API Test.\\')\\r\\n    parser.add_argument(\\'--query\\', type=str, \\r\\n                        default=\"Question: Who is the president of the US? Answer:\",\\r\\n                        help=\\'query\\') \\r\\n    parser.add_argument(\\'--model_name\\', type=str, \\r\\n                        default=\"claude-1\",\\r\\n                        help=\\'model_name\\') \\r\\n    parser.add_argument(\\'--provider\\', type=str, \\r\\n                        default=\"anthropic\",\\r\\n                        help=\\'provider\\')                         \\r\\n    args = parser.parse_args()\\r\\n    context = args.query  \\r\\n    provider = args.provider\\r\\n    model_name = args.model_name\\r\\n    print(\"test of the model service\")\\r\\n    model = make_model(provider, model_name)\\r\\n    generation = model.getcompletion(context=context,                          \\r\\n                                     use_save=False,\\r\\n                             )\\r\\n\\r\\n    print(\"---full generation is:\")\\r\\n    print(generation)\\r\\n    print(\"---generated alone--\")\\r\\n    print(generation[\"completion\"])\\r\\n    print(\"---cost---\")\\r\\n    print(generation[\\'cost\\'])\\r\\n    print(\"--end of generation\")\\r\\n    return \\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n   main()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='88abd438-227a-4664-a7f4-870bb3b9dce4', embedding=None, metadata={'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\src\\\\service\\\\utils.py', 'file_name': 'utils.py', 'file_type': 'text/x-python', 'file_size': 8688, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20', 'repo_name': 'FrugalGPT', 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\service\\\\utils.py'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# -*- coding: utf-8 -*-\\r\\n\"\"\"\\r\\nCreated on Mon Oct  3 21:06:55 2022\\r\\n\\r\\n@author: CLJ\\r\\n\"\"\"\\r\\n\\r\\nimport os, time\\r\\nimport smart_open\\r\\nimport json\\r\\nimport re\\r\\nimport urllib.request\\r\\n\\r\\nimport string\\r\\nfrom collections import Counter\\r\\n\\r\\ndef compute_cost(\\r\\n                 input_size,\\r\\n                 output_size,\\r\\n                 service_info):\\r\\n        cost = 0        \\r\\n        cost_output = service_info[\"cost_output\"]\\r\\n        cost_input = service_info[\"cost_input\"]\\r\\n        cost_fixed = service_info[\"cost_fixed\"]\\r\\n        fixed_size = service_info[\"fixed_size\"]\\r\\n        \\r\\n        cost = cost_input*input_size + cost_fixed\\r\\n        if(output_size>fixed_size):\\r\\n            cost += cost_output*(output_size - fixed_size)    \\r\\n        return cost \\r\\n\\r\\ndef debug(message,\\r\\n          debug_mode=True):\\r\\n    if(debug_mode):\\r\\n        print(message)\\r\\n\\r\\ndef load_http_text(url):\\r\\n    with urllib.request.urlopen(url) as f:\\r\\n        return f.read().decode(\"utf-8\")\\r\\n\\r\\n\\r\\ndef load_text(path):\\r\\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\\r\\n        return load_http_text(path)\\r\\n    else:\\r\\n        with smart_open.open(path) as f:\\r\\n            return f.read()\\r\\n\\r\\n\\r\\ndef load_json(path):\\r\\n    return json.loads(load_text(path))\\r\\n\\r\\n\\r\\ndef load_json_lines(path):\\r\\n    print(\"json load path:\",path)\\r\\n    return [json.loads(line) for line in load_text(path).split(\"\\\\n\") if line]\\r\\n\\r\\n\\r\\ndef dump_json(data, path):\\r\\n    with smart_open.open(path, \"w\") as f:\\r\\n        json.dump(data, f)\\r\\n\\r\\ndef dump_json_lines(data, path):\\r\\n    with smart_open.open(path, mode=\\'w\\') as f:\\r\\n        for i in range(len(data)):\\r\\n            f.write(json.dumps(data[i]))\\r\\n            f.write(\"\\\\n\")\\r\\n    \\r\\n\\r\\n\\r\\ndef dump_dataframe(df, path):\\r\\n    with smart_open.open(path, \"w\") as f:\\r\\n        df.to_csv(f, index=False)\\r\\n\\r\\n\\r\\ndef ensure_path_exists(path):\\r\\n    if \"://\" in path:\\r\\n        # Buckets like GS/S3 don\\'t need to pre-create the prefix/folder\\r\\n        return\\r\\n\\r\\n    if not os.path.exists(path):\\r\\n        os.makedirs(path)\\r\\n\\r\\n\\r\\ndef word_count(text):\\r\\n    # Count words in text, this isn\\'t well-defined but we count regex full words and\\r\\n    # single-char non-words (e.g. punctuation), similar to word tokenizers\\r\\n    return len(re.findall(r\"\\\\w+|[^\\\\w\\\\s]\", text))\\r\\n\\r\\ndef write_json(path,data):\\r\\n    json_object = json.dumps(data, indent=4)\\r\\n    with open(path,\\'w\\') as f:\\r\\n        f.write(json_object)\\r\\n    return \\r\\n\\r\\ndef evaluate_batch(answers, labels):\\r\\n    isExist = os.path.exists(\"temp/\")\\r\\n    if not isExist:        \\r\\n        # Create a new directory because it does not exist\\r\\n        os.makedirs(\"temp/\")\\r\\n    prefix = str(int(time.time()*1000000))\\r\\n    pred_path = \"temp/temp_completion_json\"+prefix+\".json\"\\r\\n    write_json(pred_path,answers)\\r\\n    true_path = \"temp/temp_true_json\"+prefix+\".json\"\\r\\n    write_json(true_path,labels)\\r\\n    metrics = eval(pred_path,true_path) \\r\\n    os.remove(true_path)\\r\\n    os.remove(pred_path)\\r\\n\\t\\r\\n    costs  = [answers[\\'cost\\'][key] for key in answers[\\'cost\\']]            \\r\\n    metrics[\\'cost_list\\'] = costs\\r\\n    metrics[\\'cost\\'] = sum(costs)\\r\\n    metrics[\\'avg_cost\\'] = sum(costs)/len(costs)\\r\\n    return metrics\\r\\n\\r\\ndef evaluate(prediction, ground_truth,metric=\"em\"):\\r\\n    if (metric == \"em\"):\\r\\n        return int(exact_match_score(prediction, ground_truth, normal_method=\"\"))\\r\\n\\r\\n\\r\\ndef normalize_answer(s,normal_method=\"\"):\\r\\n\\r\\n    def remove_articles(text):\\r\\n        return re.sub(r\\'\\\\b(a|an|the)\\\\b\\', \\' \\', text)\\r\\n\\r\\n    def white_space_fix(text):\\r\\n        return \\' \\'.join(text.split())\\r\\n\\r\\n    def remove_punc(text):\\r\\n        exclude = set(string.punctuation)\\r\\n        return \\'\\'.join(ch for ch in text if ch not in exclude)\\r\\n\\r\\n    def lower(text):\\r\\n#        print(\"text is:\",text)\\r\\n        return text.lower()\\r\\n\\r\\n    def mc_remove(text):\\r\\n        a1 = re.findall(\\'\\\\([a-zA-Z]\\\\)\\', text)\\r\\n        #print(\"text is\",text)\\r\\n        #print(\"a1\",a1)\\r\\n        if(len(a1)==0):\\r\\n            return \"\"\\r\\n        return re.findall(\\'\\\\([a-zA-Z]\\\\)\\', text)[-1]\\r\\n    if(normal_method==\"mc\"):\\r\\n        return mc_remove(s)\\r\\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\\r\\n\\r\\n\\r\\ndef f1_score(prediction, ground_truth):\\r\\n    normalized_prediction = normalize_answer(prediction)\\r\\n    normalized_ground_truth = normalize_answer(ground_truth)\\r\\n\\r\\n    ZERO_METRIC = (0, 0, 0)\\r\\n\\r\\n    if normalized_prediction in [\\'yes\\', \\'no\\', \\'noanswer\\'] and normalized_prediction != normalized_ground_truth:\\r\\n        return ZERO_METRIC\\r\\n    if normalized_ground_truth in [\\'yes\\', \\'no\\', \\'noanswer\\'] and normalized_prediction != normalized_ground_truth:\\r\\n        return ZERO_METRIC\\r\\n\\r\\n    prediction_tokens = normalized_prediction.split()\\r\\n    ground_truth_tokens = normalized_ground_truth.split()\\r\\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\\r\\n    num_same = sum(common.values())\\r\\n    if num_same == 0:\\r\\n        return ZERO_METRIC\\r\\n    precision = 1.0 * num_same / len(prediction_tokens)\\r\\n    recall = 1.0 * num_same / len(ground_truth_tokens)\\r\\n    f1 = (2 * precision * recall) / (precision + recall)\\r\\n    return f1, precision, recall\\r\\n\\r\\n\\r\\ndef exact_match_score(prediction, ground_truth, normal_method=\"\"):\\r\\n    return (normalize_answer(prediction,normal_method=normal_method) == normalize_answer(ground_truth,normal_method=normal_method))\\r\\n\\r\\n\\r\\ndef update_answer(metrics, prediction, gold):\\r\\n    em = exact_match_score(prediction, gold)\\r\\n    em_mc = exact_match_score(prediction, gold, normal_method=\"mc\")\\r\\n    #em_batch = exact_match_score(prediction, gold, normal_method=\"batch\")\\r\\n    \\'\\'\\'\\r\\n    if(em==False):\\r\\n        print(\"Not match, pred:\",prediction,\"With true:\", gold)\\r\\n    \\'\\'\\'\\r\\n    f1, prec, recall = f1_score(prediction, gold)\\r\\n    metrics[\\'em\\'] += float(em)\\r\\n    metrics[\\'em_mc\\'] += float(em_mc)\\r\\n    #metrics[\\'em_batch\\'] += float(em_batch)\\r\\n\\r\\n    metrics[\\'f1\\'] += f1\\r\\n    metrics[\\'prec\\'] += prec\\r\\n    metrics[\\'recall\\'] += recall\\r\\n    metrics[\\'em_list\\'].append(float(em))\\r\\n    metrics[\\'em_mc_list\\'].append(float(em_mc))\\r\\n    #metrics[\\'em_batch_list\\'].append(float(em_batch))\\r\\n\\r\\n    return em, prec, recall\\r\\n\\r\\ndef update_sp(metrics, prediction, gold):\\r\\n    cur_sp_pred = set(map(tuple, prediction))\\r\\n    gold_sp_pred = set(map(tuple, gold))\\r\\n    tp, fp, fn = 0, 0, 0\\r\\n    for e in cur_sp_pred:\\r\\n        if e in gold_sp_pred:\\r\\n            tp += 1\\r\\n        else:\\r\\n            fp += 1\\r\\n    for e in gold_sp_pred:\\r\\n        if e not in cur_sp_pred:\\r\\n            fn += 1\\r\\n    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\\r\\n    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\\r\\n    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\\r\\n    em = 1.0 if fp + fn == 0 else 0.0\\r\\n    metrics[\\'sp_em\\'] += em\\r\\n    metrics[\\'sp_f1\\'] += f1\\r\\n    metrics[\\'sp_prec\\'] += prec\\r\\n    metrics[\\'sp_recall\\'] += recall\\r\\n    return em, prec, recall\\r\\n\\r\\ndef eval(prediction_file, gold_file):\\r\\n    with open(prediction_file) as f:\\r\\n        prediction = json.load(f)\\r\\n    with open(gold_file) as f:\\r\\n        gold = json.load(f)\\r\\n\\r\\n    metrics = {\\'em\\': 0, \\'f1\\': 0, \\'prec\\': 0, \\'recall\\': 0,\\r\\n        \\'sp_em\\': 0, \\'sp_f1\\': 0, \\'sp_prec\\': 0, \\'sp_recall\\': 0,\\r\\n        \\'joint_em\\': 0, \\'joint_f1\\': 0, \\'joint_prec\\': 0, \\'joint_recall\\': 0,\\r\\n        \\'em_list\\':[],\\r\\n        \\'em_mc\\':0,\\r\\n        \\'em_mc_list\\':[],\\r\\n        #\\'em_batch\\':0,\\r\\n        #\\'em_batch_list\\':[],\\r\\n        }\\r\\n    for dp in gold:\\r\\n        cur_id = dp[\\'_id\\']\\r\\n        can_eval_joint = True\\r\\n        #print(prediction[\\'answer\\'])\\r\\n\\r\\n        #prediction[\\'answer\\'][cur_id]\\r\\n        if cur_id not in prediction[\\'answer\\']:\\r\\n            print(\\'missing answer {}\\'.format(cur_id))\\r\\n            can_eval_joint = False\\r\\n        else:\\r\\n            em, prec, recall = update_answer(\\r\\n                metrics, prediction[\\'answer\\'][cur_id], dp[\\'answer\\'])\\r\\n        if cur_id not in prediction[\\'sp\\']:\\r\\n            # print(\\'missing sp fact {}\\'.format(cur_id))\\r\\n            can_eval_joint = False\\r\\n        else:\\r\\n            sp_em, sp_prec, sp_recall = update_sp(\\r\\n                metrics, prediction[\\'sp\\'][cur_id], dp[\\'supporting_facts\\'])\\r\\n\\r\\n        if can_eval_joint:\\r\\n            joint_prec = prec * sp_prec\\r\\n            joint_recall = recall * sp_recall\\r\\n            if joint_prec + joint_recall > 0:\\r\\n                joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\\r\\n            else:\\r\\n                joint_f1 = 0.\\r\\n            joint_em = em * sp_em\\r\\n\\r\\n            metrics[\\'joint_em\\'] += joint_em\\r\\n            metrics[\\'joint_f1\\'] += joint_f1\\r\\n            metrics[\\'joint_prec\\'] += joint_prec\\r\\n            metrics[\\'joint_recall\\'] += joint_recall\\r\\n\\r\\n    N = len(gold)\\r\\n    for k in metrics.keys():\\r\\n        if(k !=\"em_list\" and k!=\\'em_mc_list\\'):\\r\\n            metrics[k] /= N\\r\\n\\r\\n   #print(metrics)\\r\\n    return metrics\\r\\n\\r\\n\\r\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in documents:\n",
    "    i.metadata['repo_name']=repo_name\n",
    "    i.metadata['repo_link']=repo_url+'/blob/master/'+i.metadata['file_path'][i.metadata['file_path'].find(repo_name)+len(repo_name):]\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': 'd:\\\\hackathons\\\\code_philly\\\\FrugalGPT\\\\intro.ipynb',\n",
       " 'file_name': 'intro.ipynb',\n",
       " 'file_size': 49561,\n",
       " 'creation_date': '2024-04-20',\n",
       " 'last_modified_date': '2024-04-20',\n",
       " 'repo_name': 'FrugalGPT',\n",
       " 'repo_link': 'https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\intro.ipynb'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# load data\\ndev = FrugalGPT.loadcsvdata(\"data/HEADLINES/train.csv\")\\ndev = dev[0:10]\\nprefix = open(\\'config/prompt/HEADLINES/prefix_e8.txt\\').read()\\ndata = FrugalGPT.formatdata(dev,prefix)\\n\\n\\n# Second, specify the budget per query, and then train the model. Warning: This can take a while on large datasets!\\n\\n#'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[8].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_index = vector_storing(\"./embeddings/code\",\"text-embedding-3-small\",\"vector_store\",nodes,\"VectorStoreIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_query_engine = code_index.as_query_engine(similarity_top_k=15)\n",
    "\n",
    "response = code_query_engine.query('Can I get the repo link of scoring code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The repo link of the scoring code is https://github.com/stanford-futuredata/FrugalGPT/blob/master/\\\\src\\\\FrugalGPT\\\\scoring.py'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexDict(index_id='3e80a948-d50e-456b-a292-27ffa6bd63a7', summary=None, nodes_dict={'7c382217-d342-4f00-a424-64ba3a7e4139': '7c382217-d342-4f00-a424-64ba3a7e4139', 'ff99b304-6be5-49c0-be17-52b0e30dbd99': 'ff99b304-6be5-49c0-be17-52b0e30dbd99', 'e1b33aab-2311-4f3a-80cd-513dce8a7ad0': 'e1b33aab-2311-4f3a-80cd-513dce8a7ad0', '275ea5c0-b034-43a6-abae-d12054c8dd1e': '275ea5c0-b034-43a6-abae-d12054c8dd1e', '111b6ae1-6746-400a-a5df-4397345b052e': '111b6ae1-6746-400a-a5df-4397345b052e', '729e1857-b9f5-4eb3-aa05-9f518faf9657': '729e1857-b9f5-4eb3-aa05-9f518faf9657', '5a74e21d-2abf-4798-8a5b-84d4a5bb7446': '5a74e21d-2abf-4798-8a5b-84d4a5bb7446', '441e6ee4-e27f-4df6-b5ae-760c1fede483': '441e6ee4-e27f-4df6-b5ae-760c1fede483', '939a55f2-3e1e-45f8-8be9-25ffbb505917': '939a55f2-3e1e-45f8-8be9-25ffbb505917', 'f7c643db-57f4-4eae-921e-ccecb66742c9': 'f7c643db-57f4-4eae-921e-ccecb66742c9', 'b882119b-bb08-4ff9-8d79-f3fb685ea129': 'b882119b-bb08-4ff9-8d79-f3fb685ea129', '2b632c1d-191f-4cd7-b23c-dba8334140b1': '2b632c1d-191f-4cd7-b23c-dba8334140b1', '57e89b5f-3092-47d6-bcb2-ce014221e16f': '57e89b5f-3092-47d6-bcb2-ce014221e16f', 'df706fc9-d79c-4282-9d39-36162e3d8166': 'df706fc9-d79c-4282-9d39-36162e3d8166', '85272c80-4a17-4671-b996-6d46ed66201f': '85272c80-4a17-4671-b996-6d46ed66201f', '1321c09a-2759-4bab-b986-4c5774d590e9': '1321c09a-2759-4bab-b986-4c5774d590e9', 'c69d8558-9575-4df8-9e7c-d12b63452b8a': 'c69d8558-9575-4df8-9e7c-d12b63452b8a', 'eed0891a-a53e-4a24-817f-8f5b3a3be3d3': 'eed0891a-a53e-4a24-817f-8f5b3a3be3d3', '681b95de-dd48-478b-b6e0-a16d9ebedc74': '681b95de-dd48-478b-b6e0-a16d9ebedc74', '9bf6878d-f82e-4bbf-b994-577bfe4b312b': '9bf6878d-f82e-4bbf-b994-577bfe4b312b', '13171f1f-e280-470f-9370-ded0f2233463': '13171f1f-e280-470f-9370-ded0f2233463', 'e8790c14-12ba-4df8-9626-5d7ec7ceafcf': 'e8790c14-12ba-4df8-9626-5d7ec7ceafcf', 'c2b681f2-20f8-4193-8298-306b08bebfe4': 'c2b681f2-20f8-4193-8298-306b08bebfe4', '47412249-aaa4-4e0c-a548-fcee4e4f9914': '47412249-aaa4-4e0c-a548-fcee4e4f9914', '47eddaf4-f655-4f45-9b5c-cf17aad8f92b': '47eddaf4-f655-4f45-9b5c-cf17aad8f92b', '5c0105ad-49e4-4356-a9f5-8b02413a53b9': '5c0105ad-49e4-4356-a9f5-8b02413a53b9', 'e6b51eef-549a-4e78-8544-49acb8774af1': 'e6b51eef-549a-4e78-8544-49acb8774af1', '978da585-f334-4fc6-be36-f102a89ea0b0': '978da585-f334-4fc6-be36-f102a89ea0b0', '05567c68-64c7-4a7f-a268-63edaf32bef8': '05567c68-64c7-4a7f-a268-63edaf32bef8', '8211751e-0cd4-4b22-8793-e2da6deefd69': '8211751e-0cd4-4b22-8793-e2da6deefd69', 'c57d9dde-d06b-4200-9c68-dcbc9da57996': 'c57d9dde-d06b-4200-9c68-dcbc9da57996', 'c876b0a3-7502-4f69-98a8-b3580ae823a3': 'c876b0a3-7502-4f69-98a8-b3580ae823a3', '9ff5150e-935b-49ea-9bfb-0ffc8be230d2': '9ff5150e-935b-49ea-9bfb-0ffc8be230d2', '323b9eaf-9616-409e-bf18-a178d14cd627': '323b9eaf-9616-409e-bf18-a178d14cd627', '4f28475d-6110-4cda-9094-9c27cd1c1664': '4f28475d-6110-4cda-9094-9c27cd1c1664', '7e176a77-c5f1-409a-a31c-9295c015cc51': '7e176a77-c5f1-409a-a31c-9295c015cc51', '592a5170-4853-447f-b107-c8affada7ecf': '592a5170-4853-447f-b107-c8affada7ecf', 'eeddf040-4507-4ef8-83f7-9a8f88f7312c': 'eeddf040-4507-4ef8-83f7-9a8f88f7312c', '7bc13fb2-a37a-437a-8426-6076f073b09c': '7bc13fb2-a37a-437a-8426-6076f073b09c', '4399a5ca-5eb2-446a-9b68-d9d45a57edc8': '4399a5ca-5eb2-446a-9b68-d9d45a57edc8', 'dd559c3a-de81-4ec7-82ba-5802157cbb29': 'dd559c3a-de81-4ec7-82ba-5802157cbb29', '78b6d800-c4bd-4311-9908-a074de822ec9': '78b6d800-c4bd-4311-9908-a074de822ec9', 'b402ce22-c29f-45d8-9d0e-e3124b479bd4': 'b402ce22-c29f-45d8-9d0e-e3124b479bd4', 'fd4ef7cc-5039-4a01-91aa-afb67b34894f': 'fd4ef7cc-5039-4a01-91aa-afb67b34894f', '53d86723-e85e-48ef-b39a-5554aadd0e41': '53d86723-e85e-48ef-b39a-5554aadd0e41', '5a6b6093-8b34-48bc-934b-ec682eec459e': '5a6b6093-8b34-48bc-934b-ec682eec459e', '4f777c25-1b00-40b1-a945-f455e9690bef': '4f777c25-1b00-40b1-a945-f455e9690bef', 'af800295-efff-4c8e-acd7-a3a86c7f961a': 'af800295-efff-4c8e-acd7-a3a86c7f961a', '16da77ba-bffd-4f4a-8923-4b21b82e235c': '16da77ba-bffd-4f4a-8923-4b21b82e235c', '396cc956-fe73-4d7e-a6dc-6cdb2197a579': '396cc956-fe73-4d7e-a6dc-6cdb2197a579', '1b04be66-5eaa-4271-94de-8dbcf733ba49': '1b04be66-5eaa-4271-94de-8dbcf733ba49', '161ee8bf-dfa3-4144-91a6-67a6f8cf6a24': '161ee8bf-dfa3-4144-91a6-67a6f8cf6a24', 'f2c50eac-4740-4416-82cd-fad89cb9eef1': 'f2c50eac-4740-4416-82cd-fad89cb9eef1', '18cdd906-1bd0-4bae-884a-a77435af86bb': '18cdd906-1bd0-4bae-884a-a77435af86bb', '0f91fc98-0af0-43d0-9af7-f347c2338a97': '0f91fc98-0af0-43d0-9af7-f347c2338a97', 'ceb331aa-037e-4bd9-a6e9-8785469dd983': 'ceb331aa-037e-4bd9-a6e9-8785469dd983', '879b58c3-2755-4e11-ac65-7bbbb0223781': '879b58c3-2755-4e11-ac65-7bbbb0223781', '149df5d0-867e-4eca-a1e3-6a0515ded2d8': '149df5d0-867e-4eca-a1e3-6a0515ded2d8', '5030b97c-7e0b-44f2-89a6-a27d45ce2524': '5030b97c-7e0b-44f2-89a6-a27d45ce2524', '87971c04-3994-4543-9e6f-4c706cdf6976': '87971c04-3994-4543-9e6f-4c706cdf6976', '1d1d6564-2530-4da7-b3a0-80e51c3bf38a': '1d1d6564-2530-4da7-b3a0-80e51c3bf38a', 'cd697674-997f-49e9-b8ab-d8ed6e488b62': 'cd697674-997f-49e9-b8ab-d8ed6e488b62', '0bfd4545-8706-478e-bcc4-1a3aa77a3f93': '0bfd4545-8706-478e-bcc4-1a3aa77a3f93', '9cf6f7b7-691f-42a8-a528-0c6477e744e2': '9cf6f7b7-691f-42a8-a528-0c6477e744e2', 'fa5ec3ba-e49a-4a1e-ae0a-8247bdbecd29': 'fa5ec3ba-e49a-4a1e-ae0a-8247bdbecd29', '092feabb-1538-465d-98b5-c9835a6c3cf2': '092feabb-1538-465d-98b5-c9835a6c3cf2', 'f0f1ac03-d8c3-461e-aa45-a8e8782a2fd3': 'f0f1ac03-d8c3-461e-aa45-a8e8782a2fd3', 'a2ed5835-6429-460e-ad4f-147c326cb76a': 'a2ed5835-6429-460e-ad4f-147c326cb76a', 'c5983b2b-1415-40d0-bef6-a14b599bb8da': 'c5983b2b-1415-40d0-bef6-a14b599bb8da', 'dfcd9d49-f4b4-4c41-9499-89a6d5cf8ab5': 'dfcd9d49-f4b4-4c41-9499-89a6d5cf8ab5', 'e162167b-a50a-4634-94b1-a65ccc8fa6fc': 'e162167b-a50a-4634-94b1-a65ccc8fa6fc', 'bd4cf6cc-482a-401a-8dd4-49eceb1ae531': 'bd4cf6cc-482a-401a-8dd4-49eceb1ae531', '6f68037d-a234-4ce5-8c36-c797f517a597': '6f68037d-a234-4ce5-8c36-c797f517a597', '941ca961-7c4d-4c73-85c9-bd657d3bdce0': '941ca961-7c4d-4c73-85c9-bd657d3bdce0', 'eb0c2a6f-7220-43f3-9d23-ea006cfb615f': 'eb0c2a6f-7220-43f3-9d23-ea006cfb615f', '3fca21e0-059f-4a04-a45d-bc09ae68028b': '3fca21e0-059f-4a04-a45d-bc09ae68028b', 'da3458d5-0d73-43bb-b177-ba9c82ed8ce9': 'da3458d5-0d73-43bb-b177-ba9c82ed8ce9', 'af0d11ba-ec8e-47e0-a2f6-85599b5c2d94': 'af0d11ba-ec8e-47e0-a2f6-85599b5c2d94', '1cc16855-9a1d-4742-9ff8-6e07ba0aa79b': '1cc16855-9a1d-4742-9ff8-6e07ba0aa79b', 'e3b6662a-279b-4f0e-ba94-f8f29399497f': 'e3b6662a-279b-4f0e-ba94-f8f29399497f', '366b7bc4-fa16-4bc1-95c7-f2c392416757': '366b7bc4-fa16-4bc1-95c7-f2c392416757', '261ed658-7163-4934-8441-d2bbbc0c4520': '261ed658-7163-4934-8441-d2bbbc0c4520', '8b046a7e-9e69-4155-84bc-6c7a5e542aba': '8b046a7e-9e69-4155-84bc-6c7a5e542aba', '9a7a1a8a-c051-4b58-92be-f6a6029a4633': '9a7a1a8a-c051-4b58-92be-f6a6029a4633', 'ee95c154-144b-4c77-a51f-a74976e61498': 'ee95c154-144b-4c77-a51f-a74976e61498', '72c85038-d0aa-41a8-a2ee-5064446ab137': '72c85038-d0aa-41a8-a2ee-5064446ab137', '820e8165-3931-40ef-87a3-a272197037a1': '820e8165-3931-40ef-87a3-a272197037a1', '4fcc135a-07f0-45b8-b797-ffc80827c802': '4fcc135a-07f0-45b8-b797-ffc80827c802', 'dda6a0af-b885-402d-9d91-505a19e46595': 'dda6a0af-b885-402d-9d91-505a19e46595', '91a76d57-7ea4-44fa-83f3-8110abd3378f': '91a76d57-7ea4-44fa-83f3-8110abd3378f', '5b5f910b-8b7b-4c76-b846-13321333394f': '5b5f910b-8b7b-4c76-b846-13321333394f', '524031ff-51c0-4541-9bf3-1eb51e4a9707': '524031ff-51c0-4541-9bf3-1eb51e4a9707', '29d6fde4-d4b1-4777-99a7-fab043db0730': '29d6fde4-d4b1-4777-99a7-fab043db0730', '311d9d18-e2ef-4c0b-8f2d-bec6350fae7f': '311d9d18-e2ef-4c0b-8f2d-bec6350fae7f', '4039bb4b-24b4-40c4-a711-210453921626': '4039bb4b-24b4-40c4-a711-210453921626', '7adffeba-89cd-4814-b5b1-4872168d4b75': '7adffeba-89cd-4814-b5b1-4872168d4b75', '383d281e-57a4-48c3-92d6-62be2ccfd6d1': '383d281e-57a4-48c3-92d6-62be2ccfd6d1', '356b980a-665a-4bb1-b65a-89fba21b357a': '356b980a-665a-4bb1-b65a-89fba21b357a', '7f97c546-34f3-48cf-a0bf-b0c07ed6778d': '7f97c546-34f3-48cf-a0bf-b0c07ed6778d', '7573f1fe-d631-4c77-80c5-56515fe72314': '7573f1fe-d631-4c77-80c5-56515fe72314', 'ca2db207-5fae-45c5-9964-5f7cf133de11': 'ca2db207-5fae-45c5-9964-5f7cf133de11', '6ba196d3-2c9a-412f-a8f7-0b0e1cf0eedf': '6ba196d3-2c9a-412f-a8f7-0b0e1cf0eedf', '6f72bab6-03e8-4e51-9f42-aedcc7b307db': '6f72bab6-03e8-4e51-9f42-aedcc7b307db', 'b29c18c5-93cf-4284-bf11-281ccdcfa971': 'b29c18c5-93cf-4284-bf11-281ccdcfa971', 'bae17c61-ec05-4418-b266-b6a5e3be7124': 'bae17c61-ec05-4418-b266-b6a5e3be7124', '5426767c-25ce-4f43-9432-eb228a91a33b': '5426767c-25ce-4f43-9432-eb228a91a33b', '4b74bebd-7b7b-4381-bed7-41861aa803ea': '4b74bebd-7b7b-4381-bed7-41861aa803ea', '0e397c4b-a032-4ca3-820f-1db9a461ceee': '0e397c4b-a032-4ca3-820f-1db9a461ceee', '0240ca03-2d00-4138-99b9-0cc6c1f164a4': '0240ca03-2d00-4138-99b9-0cc6c1f164a4'}, doc_id_dict={}, embeddings_dict={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_index.index_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_index = vector_load(\"./embeddings/code\",\"text-embedding-3-small\",\"vector_store\",\"VectorStoreIndex\")\n",
    "\n",
    "code_query_engine = code_index.as_query_engine(similarity_top_k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = code_query_engine.query('can you give the code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Empty Response', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexDict(index_id='8a427a3e-040c-445f-93f0-28c9e62acf5f', summary=None, nodes_dict={}, doc_id_dict={}, embeddings_dict={})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_index.index_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    splitter = CodeSplitter('python')\n",
    "\n",
    "    documents = SimpleDirectoryReader(r\"D:/Akhil_Main_LLM_Code/Akhil_Main_LLM_Code/ASRAG/pages/RAG/code\").load_data()\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "    vector_storing(\"./pages/RAG/embeddings/code\",\"text-embedding-3-small\",\"vector_store\",nodes,\"VectorStoreIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_index = vector_load(\"./embeddings/code\",\"text-embedding-3-small\",\"vector_store\",\"VectorStoreIndex\")\n",
    "\n",
    "    code_query_engine = code_index.as_query_engine(similarity_top_k=15,llm=model)\n",
    "\n",
    "\n",
    "    code_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=code_query_engine,\n",
    "        description=(\n",
    "            \"Useful for answering code related queries and provide explanation in natural language along with code\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
